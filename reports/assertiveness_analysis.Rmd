---
title: "Suggestion Assertiveness Analysis"
author: "Kenneth C. Arnold"
date: "9/18/2019"
output:
  html_document:
    self_contained: false
---

The Transparent Statistics guide, especially the [exemplars](https://transparentstats.github.io/guidelines/effectsize.html#effectsize_exemplar_within), was very helpful.

```{r setup, include=FALSE}
# https://stackoverflow.com/questions/27992239/knitr-include-figures-in-report-and-output-figures-to-separate-files
knitr::opts_chunk$set(
  echo = TRUE,
  dev=c('png', 'pdf')
)

library(dplyr)
library(readr)
library(tidyverse)
library("knitr")

# Modeling
library(lme4)
library(lmerTest)
library(car)
library(afex)
library(optimx)
library(papaja) # devtools::install_github("crsh/papaja")
library("emmeans")
#library("ARTool")
library("parallel")

# Plotting
#library("cowplot")
library("ggbeeswarm")
library("ggstance")

afex::set_sum_contrasts()

## See vignette("afex_mixed_example") for why "asymptotic".
# emm_options(lmer.df = "asymptotic") # also possible: 'satterthwaite', 'kenward-roger'
emm_options(lmer.df="kenward-roger") # slower, but needed for post-hoc tests to make sense.

```

```{r read-data, include=FALSE}
# We need to set the stimulus (image) columns to chars because they otherwise look like numbers.
allData <- read_csv(
  "../data/analyzed/combined_data.csv",
  col_types = cols(
    stimulus = col_character(),
    stimulus_order = col_character()
  ));
stop_for_problems(allData)

allData$recsVisible <- allData$condition != "norecs";
```

```{r munge-data, include=FALSE}
# Use only the "Gated" study.
allData <- allData %>% subset(experiment == "gc1");

# Exclude bad participants.
data <- subset(allData, (participant != "pr5hff") & (participant != "7q253f"))


# Set appropriate columns as nominal factors
data <- data %>% mutate_at(c('experiment', 'block', 'participant', 'stimulus', 'stimulus_order', 'condition_order'), factor);
#data$idx <- factor(data$idx)
#data$idx_in_block <- factor(data$idx_in_block)

data <- data %>% rename(
  num_predictable = corrected_bow_recs_idealuse_standard,
  chars_per_sec = characters_per_sec
);

# Name the conditions.
data <- data %>% mutate(
  condition = factor(
    recode_factor(condition, "norecs"="Never", "gated"="OnlyConfident", "standard"="Always"),
    levels = c("Never", "OnlyConfident", "Always")))

# Compute derived data.
data <- data %>% mutate(
  chars_per_sec_log = log(chars_per_sec)
);
data$itpw <- data$corrected_tapstotype_standard / data$num_words
data$itpw_log <- log(data$itpw)
data$itpw_gated <- data$corrected_tapstotype_gated / data$num_words
data$mean_rarity <- 1 - (data$mean_log_freq / 7)
data$total_nll <- -data$logprob_unconditional * data$num_words
data$tapsPerSec <- data$num_taps / data$seconds_spent_typing
data$anyErrors <- factor(data$uncorrected_errors != 0)
data$anyADJ = factor(data$ADJ != 0)

#data$num_predictable = data$corrected_bow_recs_idealuse_standard
data$frac_predictable = data$num_predictable / data$num_words

#data$num_not_recced = data$corrected_bow_recs_offered_standard - data$corrected_bow_recs_idealuse_standard
#data$frac_not_recced = 1 - data$corrected_bow_recs_idealuse_standard / data$corrected_bow_recs_offered_standard

#data <- data %>% group_by(participant) %>% mutate(mean_relevant_use_frac=mean(relevant_use_frac, na.rm=T)) %>% ungroup()
#data$rec_use_bin <- factor(ntile(data$mean_relevant_use_frac, 2))

vars.to.summarize <- list(
  "num_words"="Number of words written",
  "itpw" = "Ideal Taps per Word (ITPW)",
  "itpw_log" = "log(ITPW)",
  "itpw_gated" = "ITPW in Gated",
  "corrected_tapstotype_standard" = "Total taps to type",
  "num_chars" = "Number of characters",
  "num_colors" = "Number of colors used",
  "mean_rarity"="Mean word rarity (inverse log frequency)",
  "total_rarity"="Total rarity of words used",
  "total_nll"="Total NLL under language model",
  "ADJ" = "Fraction of adjectives",
  "NOUN" = "Fraction of nouns",
  "pos_count_ADJ" = "Number of adjectives",
  "pos_count_NOUN" = "Number of nouns",
  "pos_count_VERB" = "Number of verbs",
  "num_predictable" = "Number of predictable words",
  "frac_predictable" = "Fraction of words that were predictable",
  "relevant_use_frac" = "Fraction of relevant predictions that were used",
  "chars_per_sec"="Characters per Second",
  "tapsPerSec" = "Taps per Second",
  "TLX_sum"="Total cognitive load (TLX)",
  "physical"="Physical load (TLX)",
  "mental"="Mental load (TLX)"
)

expLevel <- data %>%
  group_by(experiment, condition_order, participant, age, chars_per_sec_norecs_mean, steppedBack, gender, use_predictive) %>%
  summarise(
    chars_per_sec_mean = mean(chars_per_sec),
    rec_use_per_seen_mean=mean(rec_use_per_seen, na.rm=TRUE))

data <- left_join(data, expLevel, copy=TRUE)

longForm <- data %>%
  select(experiment, participant, condition, block, idx, idx_in_block, steppedBack, !!!names(vars.to.summarize)) %>%
  gather(measure, value, !!!names(vars.to.summarize)) %>%
  mutate(measure=dplyr::recode(measure, !!!vars.to.summarize))

```

# Summaries

Overall:
```{r}
print(paste("Total", n_distinct(allData$participant), "participants"))
```

Gender:
```{r genders}
# Gender distribution
allData %>%
  group_by(gender) %>%
  summarize(count=n_distinct(participant)) %>%
  spread(gender, count)
#counts.gc1 <- filter(genderCounts, experiment=="gc1")
```

Ages:
```{r ages}
# Age range
allData %>%
  summarize(min=min(age), max=max(age))
```

# Adjustments

```{r typosByCondition}
xt <- xtabs(~ condition + anyErrors, data=data)
#typo_test <- fisher.test(xt)
xt %>% kable()
```
```{r typoDifferenceSignificant}
xt <- xtabs(~ recsVisible + anyErrors, data=data)
fisher.test(xt)
```


```{r}
res <- lmer(num_predictable ~ condition + (1|participant) + (1|stimulus), data=data)
p <- pairs(emmeans(res, "condition"))
```

```{r}
# based on https://transparentstats.github.io/guidelines/effectsize.html#effectsize_exemplar_within
# Sample subjects, but keep all data for each.
getBootstrapSample <- function () {
  subjects <- unique(data$participant)
  sampled_subjects <- sample(subjects, length(subjects), replace = TRUE)
  get_data_for_subject <- function(i) {
    data %>%
      # extract that subject's data
      filter(participant == sampled_subjects[i]) %>%
      # give them a unique id
      mutate(participant = paste0(sampled_subjects[i], '_', i))
  };
  # Need bind_rows to concatenate the samples for each subject.
  lapply(1:length(sampled_subjects), get_data_for_subject) %>% bind_rows()
};

analyze_one_iteration <- function(measures) {
  sampled <- getBootstrapSample();
  lapply(measures, function(m) {
    paste0(m, " ~ condition + (1|participant) + (1|stimulus)") %>% 
    as.formula() %>%
    lme4::lmer(data=sampled) %>% 
    emmeans(specs="condition") %>% 
    pairs() %>% 
    as.data.frame(infer=F) %>%
    select(contrast, estimate) %>% 
    mutate(measure=m)
  }) %>% bind_rows()
}

#analyze_one_iteration()

analyze_n_iterations <- function(n_iterations, measures) {
  lapply(1:n_iterations, function(x) analyze_one_iteration(measures)) %>% bind_rows()
}

#analyze_n_iterations(
#  measures = c("num_predictable", "frac_predictable", "num_words"),
#  n_iterations = 1)
```

```{r}
core_count <- detectCores()
cluster <- makeCluster(core_count)
clusterSetRNGStream(cluster, iseed = 0)

measures_to_analyze = c("num_predictable", "frac_predictable", "num_words")

clusterExport(cluster, varlist = c("getBootstrapSample", "analyze_one_iteration", "analyze_n_iterations", "data"))
clusterCall(cluster, function() {
  library("tidyverse")
  library("lme4")
  library("emmeans")
  NULL
});


iterations_per_node <- ceiling(1000/core_count)
system.time(
  boot_results <- parLapply(
    cluster,
    rep(iterations_per_node, core_count),
    analyze_n_iterations,
    rep( measures_to_analyze, core_count)
    )
)

stopCluster(cluster);
```

```{r}
# inner 95%
PERCENTILE_LO <- 0.025
PERCENTILE_HI <- 0.975

confints <- boot_results %>% 
  bind_rows %>% 
  group_by(measure, contrast) %>% 
  summarize(
    mean=mean(estimate),
    conf_low = unname(quantile(estimate, probs = PERCENTILE_LO)),
    conf_high = unname(quantile(estimate, probs = PERCENTILE_HI)))

```

```{r}
confints %>%
  ggplot(aes(x=contrast, y=mean, ymin=conf_low, ymax=conf_high)) +
  geom_hline(yintercept = 0, linetype ='dotted') +
  geom_pointrange() +
  expand_limits(x=0) +
  labs(x="", y="Estimated pairwise difference") +
  coord_flip() +
  facet_wrap(vars(measure), ncol=1, scales = "free")
```

```{r}
confints %>%
  ggplot(aes(x=contrast, y=mean, ymin=conf_low, ymax=conf_high)) +
  geom_hline(yintercept = 0, linetype ='dotted') +
  geom_pointrange() +
  expand_limits(x=0) +
  labs(x="", y="Estimated pairwise difference") +
  coord_flip() +
  facet_grid(measure~., scales = "free")
```


# Subtract per-image mean, aggregate by block.

```{r}
dataMinusImgMean <- data %>%
  group_by(stimulus) %>% 
  mutate(
    num_predictable = num_predictable - mean(num_predictable),
    frac_predictable = frac_predictable - mean(frac_predictable),
    num_words = num_words - mean(num_words),
    chars_per_sec = chars_per_sec - mean(chars_per_sec),
    chars_per_sec_log = chars_per_sec_log - mean(chars_per_sec_log)
   ) %>% 
  ungroup();

blockLevel <- dataMinusImgMean %>% 
  group_by(participant, condition, block) %>% 
  summarise_at(c(
    'num_predictable', 'frac_predictable', 'num_words',
    'chars_per_sec', 'chars_per_sec_log', 'chars_per_sec_norecs_mean', 'chars_per_sec_ratio_to_norecs',
    'pos_count_NOUN', 'pos_count_ADJ', 'num_colors'
    ), mean);
#    num_predictable=mean(num_predictable),
#    frac_predictable=mean(frac_predictable),
#    num_words=mean(num_words),
#    chars_per_sec=mean(chars_per_sec))
```

```{r}
labeledBar <- list(
  stat_summary(geom="bar", fun.y=mean, position="dodge"),
    stat_summary(geom="errorbar", fun.data=mean_se, position=position_dodge(width=.9), width=.4),
    #scale_y_continuous(expand=expand_scale(mult=c(0,.05))), # remove the extra spacing at 0
    stat_summary(geom="text", fun.y=mean,
                 aes(label=sprintf("%1.2f", ..y..)),
                 position=position_dodge(width=.9),
                 hjust="middle", vjust="bottom"))
```

# RQ1: To what degree do people choose the words that the system suggests?

```{r num_predictable}
blockLevel %>%
  #group_by(participant) %>% mutate(num_predictable = num_predictable - mean(num_predictable)) %>% ungroup() %>% 
  ggplot(aes(x=condition, y=num_predictable)) + 
  #geom_boxplot() +
  labeledBar + 
  #stat_summary(geom="bar", fun.data=mean_se) +
  #stat_summary(geom="errorbar", fun.data=mean_se)
  coord_flip()
```

```{r frac_predictable}
blockLevel %>% ggplot(aes(x=condition, y=frac_predictable)) + labeledBar + coord_flip()
```


```{r "examples"}
total <- data %>% group_by(stimulus) %>% count() %>% pull(n) %>% mean()
q1 <- floor(total * .25)
q3 <- floor(total * .75)

sorted <- data %>%
  group_by(stimulus) %>%
  arrange(frac_predictable)

sorted %>% 
  slice(q1) %>%
  select(stimulus, frac_predictable, corrected_text) %>% 
  kable(format="latex") %>% 
  cat(sep="\n");

sorted %>% 
  group_by(stimulus) %>%
  arrange(frac_predictable) %>%
  slice(q3) %>%
  select(stimulus, frac_predictable, corrected_text) %>% 
  kable(format="latex") %>% 
  cat(sep="\n");


```

```{r "predictabilityVsLength"}
data %>% 
  ggplot(aes(x=num_words, y=frac_predictable)) +
  stat_smooth(method = "lm") +
  geom_point()
```

```{r individualDiffs}
getPairwiseDiffs <- function(colName) {
  blockLevel %>%
    pivot_wider(id_cols = c("participant"), names_from = "condition", values_from = colName) %>% 
    mutate(
      `Always-Never` = Always - Never,
      `Always-OnlyConfident` = Always - OnlyConfident,
      `OnlyConfident-Never` = OnlyConfident - Never
    )
};

predictable_pdiffs <- getPairwiseDiffs("num_predictable")

pdiffs <- bind_rows(
  num_predictable = getPairwiseDiffs("num_predictable"),
  frac_predictable = getPairwiseDiffs("frac_predictable"),
  num_words = getPairwiseDiffs("num_words"),
  .id = "measure"
)

#predictable_pdiffs %>% 
#  ggplot(aes(x=`Extra-Intro`)) + geom_dotplot()

pdiffs %>% 
  pivot_longer(c('Always-Never', 'Always-OnlyConfident', 'OnlyConfident-Never')) %>% 
  ggplot(aes(x=value, y=name)) + ggstance::geom_boxploth() + facet_wrap(vars(measure), ncol = 1, scales = "free")
```


```{r}
results <- afex::aov_ez(
  data=blockLevel,
  id="participant", # subject identifier
  dv="num_predictable", # outcome
  within=c('condition'),
  between=NULL,
  fun_aggregate = mean,
  anova_table = list(es='ges'));

#select(-`Pr(>F)`)
#results$anova_table %>% as_tibble(rownames = "effect")
results
```

```{r}
pairs(emmeans(results, "condition"), comparisons = TRUE)
```


```{r}
afex::aov_ez(
  data=blockLevel,
  id="participant",
  dv="frac_predictable",
  within=c('condition'),
  between=NULL);
```

# How do predictions affect text length?

```{r num_words}
blockLevel %>% ggplot(aes(x=condition, y=num_words)) + labeledBar + coord_flip()
```

```{r num_words_analysis}
afex::aov_ez(
  data=blockLevel,
  id="participant",
  dv="num_words",
  within=c('condition'),
  between=NULL);
```


# How does text entry speed depend on suggestion presence and assertiveness?

```{r speed}
blockLevel %>% ggplot(aes(x=condition, y=chars_per_sec)) + labeledBar + coord_flip()
```

```{r speedAnalysis}
afex::mixed(chars_per_sec_log ~ condition * block + (1|participant), data=blockLevel)
```

```{r speedAnalysisMANOVA}
afex::aov_ez(
  data=blockLevel,
  id="participant",
  dv="chars_per_sec",
  within=c('condition')
)
```

```{r speed_interaction}
blockLevel %>%
  ggplot(aes(x=chars_per_sec_norecs_mean, y=chars_per_sec_ratio_to_norecs, color=condition)) +
  geom_smooth()# +
  #facet_wrap(vars(block))
```

```{r speedInteractionAnalysis}
afex::mixed(chars_per_sec_log ~ condition * block * log(chars_per_sec_norecs_mean) + (1|participant), data=blockLevel)
```

# Supplemental Analyses

## Part of Speech

```{r contentExploratory}
multiBar <- function(measures.to.vis, nrow=3) {
  blockLevel %>%
    ungroup() %>%
    select(participant, condition, block, !!!measures.to.vis) %>%
    gather(measure, value, !!!measures.to.vis) %>%
    mutate(measure=factor(measure, levels=measures.to.vis)) %>%
    mutate(measure=dplyr::recode(measure, !!!vars.to.summarize)) %>%
    ggplot(aes(x=condition, y=value)) +
      labeledBar +
      facet_wrap(vars(measure), nrow=3, scales="free", strip.position="top") +
      labs(x="Prediction assertiveness", y="") +
      coord_flip() +
      theme(
        strip.background = element_blank(),
        panel.spacing.y = unit(2, "lines"),
        axis.line.x = element_blank(),
        legend.position = "bottom")
}

multiBar(c("pos_count_NOUN", "pos_count_ADJ", "num_colors"))
```
