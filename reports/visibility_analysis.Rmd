---
title: "Suggestion Visibility Analysis"
author: "Kenneth C. Arnold"
date: "9/18/2019"
output:
  html_document:
    self_contained: false
---

The Transparent Statistics guide, especially the [exemplars](https://transparentstats.github.io/guidelines/effectsize.html#effectsize_exemplar_within), was very helpful.

```{r setup, include=FALSE}
# https://stackoverflow.com/questions/27992239/knitr-include-figures-in-report-and-output-figures-to-separate-files
knitr::opts_chunk$set(
  echo = TRUE,
  dev=c('png', 'pdf')
)

library(dplyr)
library(readr)
library("knitr")

# Modeling
library(lme4)
library(lmerTest)
library(car)
#library(optimx)
#library("ARTool")
library("parallel")

# Plotting
#library("cowplot")
library("ggbeeswarm")
library("ggstance")

doAnalysisSetup <- function() {
  library("tidyverse")
  library("afex")
  library("emmeans")  
  afex::set_sum_contrasts()

  ## See vignette("afex_mixed_example") for why "asymptotic".
  emm_options(lmer.df = "asymptotic") # also possible: 'satterthwaite', 'kenward-roger'
  #emm_options(lmer.df="kenward-roger") # slower, but needed for post-hoc tests to make sense.
};
doAnalysisSetup()
```

```{r read-data, include=FALSE}
# We need to set the stimulus (image) columns to chars because they otherwise look like numbers.
allData <- read_csv(
  "../data/analyzed/combined_data.csv",
  col_types = cols(
    stimulus = col_character(),
    stimulus_order = col_character()
  ));
stop_for_problems(allData)

allData$recsVisible <- allData$condition != "norecs";
```

```{r munge-data, include=FALSE}
# Use only the "Gated" study.
allData <- allData %>% subset(experiment == "gc1");

# Exclude bad participants.
data <- subset(allData, (participant != "pr5hff") & (participant != "7q253f"))


# Set appropriate columns as nominal factors
data <- data %>% mutate_at(c('experiment', 'block', 'participant', 'stimulus', 'stimulus_order', 'condition_order'), factor);
#data$idx <- factor(data$idx)
#data$idx_in_block <- factor(data$idx_in_block)

data <- data %>% rename(
    num_predictable = corrected_bow_recs_idealuse_standard,
    chars_per_sec = characters_per_sec) %>%
  mutate(
    frac_predictable = num_predictable / num_words,
    num_unpredictable = num_words - num_predictable
  );


# Name the conditions.
data <- data %>% mutate(
  condition = factor(
    recode_factor(condition, "norecs"="Never", "gated"="OnlyConfident", "standard"="Always"),
    levels = c("Never", "OnlyConfident", "Always")))

# Compute derived data.
data <- data %>% mutate(
  chars_per_sec_log = log(chars_per_sec)
);
data$itpw <- data$corrected_tapstotype_standard / data$num_words
data$itpw_log <- log(data$itpw)
data$itpw_gated <- data$corrected_tapstotype_gated / data$num_words
data$mean_rarity <- 1 - (data$mean_log_freq / 7)
data$total_nll <- -data$logprob_unconditional * data$num_words
data$tapsPerSec <- data$num_taps / data$seconds_spent_typing
data$anyErrors <- factor(data$uncorrected_errors != 0)
data$anyADJ = factor(data$ADJ != 0)


#data$num_not_recced = data$corrected_bow_recs_offered_standard - data$corrected_bow_recs_idealuse_standard
#data$frac_not_recced = 1 - data$corrected_bow_recs_idealuse_standard / data$corrected_bow_recs_offered_standard

#data <- data %>% group_by(participant) %>% mutate(mean_relevant_use_frac=mean(relevant_use_frac, na.rm=T)) %>% ungroup()
#data$rec_use_bin <- factor(ntile(data$mean_relevant_use_frac, 2))

vars.to.summarize <- list(
  "num_words"="Number of words written",
  "itpw" = "Ideal Taps per Word (ITPW)",
  "itpw_log" = "log(ITPW)",
  "itpw_gated" = "ITPW in Gated",
  "corrected_tapstotype_standard" = "Total taps to type",
  "num_chars" = "Number of characters",
  "num_colors" = "Number of colors used",
  "mean_rarity"="Mean word rarity (inverse log frequency)",
  "total_rarity"="Total rarity of words used",
  "total_nll"="Total NLL under language model",
  "ADJ" = "Fraction of adjectives",
  "NOUN" = "Fraction of nouns",
  "pos_count_ADJ" = "Number of adjectives",
  "pos_count_NOUN" = "Number of nouns",
  "pos_count_VERB" = "Number of verbs",
  "num_predictable" = "Number of predictable words",
  "frac_predictable" = "Fraction of words that were predictable",
  "relevant_use_frac" = "Fraction of relevant predictions that were used",
  "chars_per_sec"="Characters per Second",
  "tapsPerSec" = "Taps per Second",
  "TLX_sum"="Total cognitive load (TLX)",
  "physical"="Physical load (TLX)",
  "mental"="Mental load (TLX)"
)

# expLevel <- data %>%
#   group_by(experiment, condition_order, participant, age, chars_per_sec_norecs_mean, steppedBack, gender, use_predictive) %>%
#   summarise(
#     chars_per_sec_mean = mean(chars_per_sec),
#     rec_use_per_seen_mean=mean(rec_use_per_seen, na.rm=TRUE))
# 
# data <- left_join(data, expLevel, copy=TRUE)
# 
# longForm <- data %>%
#   select(experiment, participant, condition, block, idx, idx_in_block, steppedBack, !!!names(vars.to.summarize)) %>%
#   gather(measure, value, !!!names(vars.to.summarize)) %>%
#   mutate(measure=dplyr::recode(measure, !!!vars.to.summarize))

```

```{r helpfulRank}
# I want:
#  participant | aspect | condition | rank
#
# I do that in a few steps:
# 1. Join the separate columns into one. helpfulAccurate="norecs-gated"
# 2. Split out the 

# 1. Select only the helpfulRank columns.
expHelpful <- data %>%
  select(participant, starts_with("helpfulRank")) %>% 
  select(participant, ends_with("condition")) %>% 
  group_by(participant) %>% summarise_all(first)

# Tidy the data (participant, aspect, extreme, value)
helpfulTidy <- expHelpful %>% 
  pivot_longer(
    cols=starts_with("helpfulRank"),
    names_prefix="helpfulRank-",
    names_sep="-",
    names_to = c("aspect", "extreme", "unused"),
    values_to = "condition"
  ) %>% select(-unused)
  # concatenate the two values
#  group_by(participant, aspect) %>% summarize(val_concat=decode(extreme, value))#paste(value, sep="-", collapse=""))
#data %>% transmute(
#  helpfulAccurate=paste(`helpfulRank-accurate-least-condition`, `helpfulRank-accurate-most-condition`, sep="-"),
#  )
#helpfulTidy %>% mutate(picked=TRUE) %>% pivot_wider(id_cols = c("participant", "aspect"), names_from=c("extreme", "value"), values_from=picked, values_fill = list(picked=FALSE))
helpfulCounts <- helpfulTidy %>% subset(extreme=="most") %>% group_by(condition) %>% summarize(picked=n())
helpfulCounts
```

```{r helpfulChiSq}
#helpfulTidy %>% pivot_wider(id_cols = )
chisq.test(helpfulCounts$picked)
```
# Summaries

Overall:
```{r}
print(paste("Total", n_distinct(allData$participant), "participants,", n_distinct(data$participant), "after exclusion"))
print(paste(nrow(data), "captions"))
```

Gender:
```{r genders}
# Gender distribution
allData %>%
  group_by(gender) %>%
  summarize(count=n_distinct(participant)) %>%
  spread(gender, count)
#counts.gc1 <- filter(genderCounts, experiment=="gc1")
```

Ages:
```{r ages}
# Age range
allData %>%
  summarize(min=min(age), max=max(age))
```

# Adjustments

```{r typosByCondition}
xt <- xtabs(~ condition + anyErrors, data=data)
#typo_test <- fisher.test(xt)
xt %>% addmargins() %>% kable()
```
```{r typoDifferenceSignificant}
xt <- xtabs(~ recsVisible + anyErrors, data=data)
#xt <- xtabs(~ condition + anyErrors, data=data)
fisher.test(xt)
```

# Set Up Analysis Task

```{r "blockLevelAggregation"}
blockLevelAll <- data %>% 
  group_by(participant, condition, block) %>%
  summarise_at(c("TLX_sum", "physical", "mental"), first) %>% 
  filter(!(any(is.na(TLX_sum)))) %>% 
  ungroup()

blockLevelRecsOnly <- subset(blockLevel, condition != "Never")

```

```{r bootstrapGenTask}
total_iterations <- 10000

saveRDS(
  list(
    name="Trial-Level",
    iterations=total_iterations,
    data=data,
    measures=list(
      list("num_predictable", " ~ condition + (1|participant) + (1|stimulus)"),
      list("frac_predictable", " ~ condition + (1|participant) + (1|stimulus)"),
      list("num_words", " ~ condition + (1|participant) + (1|stimulus)"))
  ),
  "tasks/trial_level.rds")

saveRDS(
  list(
    name="Block-Level All",
    iterations=total_iterations,
    data=blockLevelAll,
    measures=list(
      list("physical", " ~ condition + (1|participant)"),
      list("mental", " ~ condition + (1|participant)"),
      list("TLX_sum", " ~ condition + (1|participant)")
    )
  ),
  "tasks/block_level_all.rds"
)

saveRDS(
  list(
    name="Block-Level Suggs",
    iterations=total_iterations,
    data=blockLevelRecsOnly,
    measures=list(
      list("chars_per_sec_ratio_to_norecs", " ~ condition + (1|participant)")
    )
  ),
  "tasks/block_level_suggs.rds"
)
```

```{r loadResults}
trial_level_results <- readRDS("task_results/trial_level.rds")

```

```{r}
# inner 95%
PERCENTILE_LO <- 0.025
PERCENTILE_HI <- 0.975

confints <- trial_level_results %>% 
  group_by(type, measure, statistic) %>% 
  summarize(
    n=n(),
    mean=mean(estimate),
    conf_low = unname(quantile(estimate, probs = PERCENTILE_LO)),
    conf_high = unname(quantile(estimate, probs = PERCENTILE_HI)))

```

```{r bootstrap_lsmeans}
confints %>%
  subset(type == "means") %>% 
  mutate(statistic=factor(statistic, levels=c("Always", "OnlyConfident", "Never"))) %>% 
  ggplot(aes(x=statistic, y=mean, ymin=conf_low, ymax=conf_high)) +
  geom_hline(yintercept = 0, linetype ='dotted') +
  geom_pointrange() +
  geom_text(aes(label=sprintf("%1.2f", ..y..)), nudge_x=.3, hjust="middle", vjust="bottom", size=2) +
  geom_text(aes(label=sprintf("%1.2f", conf_low), y=conf_low), hjust="right", size=2) +
  geom_text(aes(label=sprintf("%1.2f", conf_high), y=conf_high), hjust="left", size=2) +
  expand_limits(x=0) +
  labs(x="", y="Least-Squares Means") +
  coord_flip() +
  facet_wrap(vars(measure), ncol=1, scales = "free")
```

```{r bootstrap_pairwise}
# The "g" format string uses sigfigs, rather than after decimal point, for frac_predicted.
confints %>%
  subset(type == "pairs") %>% 
  ggplot(aes(x=statistic, y=mean, ymin=conf_low, ymax=conf_high)) +
  geom_hline(yintercept = 0, linetype ='dotted') +
  geom_pointrange() +
  geom_text(aes(label=sprintf("%1.2f", ..y..)), nudge_x=.3, hjust="middle", vjust="bottom", size=2) +
  geom_text(aes(label=sprintf("%1.2g", conf_low), y=conf_low), hjust="right", size=2) +
  geom_text(aes(label=sprintf("%1.2g", conf_high), y=conf_high), hjust="left", size=2) +
  expand_limits(x=0) +
  labs(x="", y="Estimated pairwise difference") +
  coord_flip() +
  facet_wrap(vars(measure), ncol=1, scales = "free")
```

# Subtract per-image mean, aggregate by block.

```{r}
dataMinusImgMean <- data %>%
  group_by(stimulus) %>% 
  mutate(
    num_predictable = num_predictable - mean(num_predictable),
    frac_predictable = frac_predictable - mean(frac_predictable),
    num_words = num_words - mean(num_words),
    chars_per_sec = chars_per_sec - mean(chars_per_sec),
    chars_per_sec_log = chars_per_sec_log - mean(chars_per_sec_log)
   ) %>% 
  ungroup();

blockLevel <- dataMinusImgMean %>% 
  group_by(participant, condition, block) %>% 
  summarise_at(c(
    'num_predictable', 'frac_predictable', 'num_words',
    'chars_per_sec', 'chars_per_sec_log', 'chars_per_sec_norecs_mean', 'chars_per_sec_ratio_to_norecs',
    'pos_count_NOUN', 'pos_count_ADJ', 'num_colors'
    ), mean);
#    num_predictable=mean(num_predictable),
#    frac_predictable=mean(frac_predictable),
#    num_words=mean(num_words),
#    chars_per_sec=mean(chars_per_sec))
```

```{r}
labeledBar <- list(
  stat_summary(geom="bar", fun.y=mean, position="dodge"),
    stat_summary(geom="errorbar", fun.data=mean_se, position=position_dodge(width=.9), width=.4),
    #scale_y_continuous(expand=expand_scale(mult=c(0,.05))), # remove the extra spacing at 0
    stat_summary(geom="text", fun.y=mean,
                 aes(label=sprintf("%1.2f", ..y..)),
                 position=position_dodge(width=.9),
                 hjust="middle", vjust="bottom"))
```


```{r "examples"}
total <- data %>% group_by(stimulus) %>% count() %>% pull(n) %>% mean()
q1 <- floor(total * .25)
q3 <- floor(total * .75)

sorted <- data %>%
  group_by(stimulus) %>%
  arrange(frac_predictable)

sorted %>% 
  slice(q1) %>%
  select(stimulus, frac_predictable, corrected_text) %>% 
  kable(format="latex") %>% 
  cat(sep="\n");

sorted %>% 
  group_by(stimulus) %>%
  arrange(frac_predictable) %>%
  slice(q3) %>%
  select(stimulus, frac_predictable, corrected_text) %>% 
  kable(format="latex") %>% 
  cat(sep="\n");


```


```{r speed_interaction}
blockLevelRecsOnly %>%
  ggplot(aes(x=chars_per_sec_norecs_mean, y=chars_per_sec_ratio_to_norecs, color=condition)) +
  geom_point(alpha=.5) +
  geom_smooth(method="loess") +
  labs(
    x="Baseline Speed (chars/sec in trials with with Hidden suggestions)",
    y="Block-level mean ratio of speed in trial to baseline speed",
    color="Suggestion Visibility")
  #facet_wrap(vars(block))
```

```{r speedInteractionSimple}
lmer(
  chars_per_sec_ratio_to_norecs ~ chars_per_sec_norecs_mean + (1|participant),
  data=blockLevelRecsOnly) %>% 
confint.merMod(
  nsim = 1000,
  parallel="multicore",
  method = "boot")
```

```{r speedInteractionWithCondition}
lmer(
  chars_per_sec_ratio_to_norecs ~ condition *  chars_per_sec_norecs_mean + (1|participant),
  data=blockLevelRecsOnly) %>%
confint.merMod(
  nsim = 1000,
  parallel="multicore",
  method = "boot")
```

# How did subjective experience depend on suggestion visibility?

```{r TLXconfitns}
# inner 95%
PERCENTILE_LO <- 0.025
PERCENTILE_HI <- 0.975

TLXconfints <- boot_results %>% 
  bind_rows %>% 
  group_by(type, measure, statistic) %>% 
  summarize(
    n=n(),
    mean=mean(estimate),
    conf_low = unname(quantile(estimate, probs = PERCENTILE_LO)),
    conf_high = unname(quantile(estimate, probs = PERCENTILE_HI)))

```

```{r TLX}


# tlx.boot <- boot::boot(
#   data=unique(tlxData$participant),
#   parallel = "multicore",
#   ncpus = detectCores(),
#   R=100,
#   statistic=function(subjects, indices) {
#    bootstrap_sample <- lapply(1:length(indices), function(i) {
#      idx <- indices[i]
#      tlxData %>%
#        filter(participant == subjects[idx]) %>%
#        mutate(participant=paste0(subjects[idx], '_', i))
#    }) %>% bind_rows()
# #   mean(bootstrap_sample$TLX_sum)
#    model <- lme4::lmer(TLX_sum ~ condition + block + (1|participant), data=bootstrap_sample)
#    model %>% emmeans(specs="condition") %>% 
#       {list(
#         means=emmeans_to_stats(.)
#       , pairs=pairs_to_stats(.))} %>% bind_rows(.id="type") %>% 
#     mutate(statistic=factor(statistic))
#   }
# )
if(F){
emm_options(lmer.df="kenward-roger")
tlxModel <- lmer(TLX_sum ~ condition + block + (1|participant), data=tlxData)
tlxCIs <- pairs(emmeans(tlxModel, specs = c("condition"))) %>% as.data.frame(infer=c(TRUE, FALSE))  %>% 
  rename(mean=estimate, conf_low=lower.CL, conf_high=upper.CL)
tlxCIs}
```

```{r tlxPairwise}
#, fig.width=7, fig.height=1.5}
# Maybe: https://stackoverflow.com/questions/45361904/duplicating-and-modifying-discrete-axis-in-ggplot2/45362497#45362497
#tlxCIs %>% #separate(contrast, into=c("right", "left"), remove=FALSE) %>% 
TLXconfints %>% 
  subset(type == "pairs") %>% 
  ggplot(aes(x=statistic, y=mean, ymin=conf_low, ymax=conf_high)) +
  geom_hline(yintercept = 0, linetype ='dotted') +
  geom_pointrange() +
  geom_text(aes(label=sprintf("%1.2f", ..y..)), nudge_x=.3, hjust="middle", vjust="bottom", size=2) +
  geom_text(aes(label=sprintf("%1.2g", conf_low), y=conf_low), hjust="right", size=2) +
  geom_text(aes(label=sprintf("%1.2g", conf_high), y=conf_high), hjust="left", size=2) +
  #expand_limits(x=0) +
  labs(x="", y="Estimated increase in TLX load") +
  coord_flip() +
  facet_wrap(vars(measure), ncol=1, scales = "free")
#ggsave("TLXpairwise.pdf");
```

# Supplemental Analyses

## Part of Speech

```{r contentExploratory}
multiBar <- function(measures.to.vis, nrow=3) {
  blockLevel %>%
    ungroup() %>%
    select(participant, condition, block, !!!measures.to.vis) %>%
    gather(measure, value, !!!measures.to.vis) %>%
    mutate(measure=factor(measure, levels=measures.to.vis)) %>%
    mutate(measure=dplyr::recode(measure, !!!vars.to.summarize)) %>%
    ggplot(aes(x=condition, y=value)) +
      labeledBar +
      facet_wrap(vars(measure), nrow=3, scales="free", strip.position="top") +
      labs(x="Prediction visibility", y="") +
      coord_flip() +
      theme(
        strip.background = element_blank(),
        panel.spacing.y = unit(2, "lines"),
        axis.line.x = element_blank(),
        legend.position = "bottom")
}

multiBar(c("pos_count_NOUN", "pos_count_ADJ", "num_colors"))
```
