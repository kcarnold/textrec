{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ~/src/DiscCaptioning/\n",
    "# %run eval.py --decoding_constraint 1 --dump_images 0 --num_images -1 --batch_size 50 --split test  --input_label_h5 data/cocotalk_label.h5 --input_fc_dir data/cocotalk_fc --input_att_dir data/cocobu_att --model log_att_d1/model.pth --beam_size 5 --temperature 1.0 --sample_max 1 --infos_path log_att_d1/infos_att_d1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import toolz\n",
    "from IPython.display import Image, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO captions\n",
      "Loading COCO id2url\n",
      "Done\n",
      "Loading COCO captions\n",
      "Loading COCO id2url\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<module 'textrec.analysis_util' from '/Users/kcarnold/code/textrec/src/textrec/analysis_util.py'>,\n",
       " <module 'textrec.util' from '/Users/kcarnold/code/textrec/src/textrec/util.py'>,\n",
       " <module 'textrec.notebook_util' from '/Users/kcarnold/code/textrec/src/textrec/notebook_util.py'>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textrec.paths import paths\n",
    "from textrec import analysis_util, util, notebook_util\n",
    "reload(analysis_util), reload(util), reload(notebook_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = notebook_util.images\n",
    "id2img = notebook_util.id2img\n",
    "id2url = notebook_util.id2url\n",
    "images_by_split = notebook_util.images_by_split\n",
    "show_images = notebook_util.show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer, valid_img_by_word = util.get_vectorized_captions(split='val')\n",
    "cap_vectorizer, caption_vecs = util.get_vectorized_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images(caption, n=10):\n",
    "    query_vec = cap_vectorizer.transform([caption])\n",
    "    similarity = caption_vecs.dot(query_vec.T).A.ravel()\n",
    "    return [images[idx]['cocoid'] for idx in np.argsort(similarity)[-n:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a cat sitting on a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: inline-block;\"><div>train/462767</div><img src=\"http://images.cocodataset.org/train2017/000000462767.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/299933</div><img src=\"http://images.cocodataset.org/train2017/000000299933.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/128429</div><img src=\"http://images.cocodataset.org/train2017/000000128429.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/329734</div><img src=\"http://images.cocodataset.org/train2017/000000329734.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>test/45208</div><img src=\"http://images.cocodataset.org/train2017/000000045208.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/108751</div><img src=\"http://images.cocodataset.org/train2017/000000108751.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/449599</div><img src=\"http://images.cocodataset.org/train2017/000000449599.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/143123</div><img src=\"http://images.cocodataset.org/train2017/000000143123.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/580234</div><img src=\"http://images.cocodataset.org/train2017/000000580234.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/412749</div><img src=\"http://images.cocodataset.org/train2017/000000412749.jpg\" style=\"max-width: 200px\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_caption = 'a cat sitting on a'\n",
    "print(query_caption)\n",
    "image_set = get_similar_images(query_caption)\n",
    "HTML(show_images(image_set, show_captions=False, show_label=True, max_width=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a next-word recommendation that distinguishes one image from the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import onmt.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONMT models...\n",
      "coco_lm_adam_acc_46.00_ppl_16.32_e10_nooptim.pt\n",
      "Loading model parameters.\n",
      "coco_cap_adam_acc_48.73_ppl_12.56_e10_nooptim.pt\n",
      "Loading model parameters.\n",
      "Ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'textrec.onmt_model_2' from '/Users/kcarnold/code/textrec/src/textrec/onmt_model_2.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textrec import onmt_model_2\n",
    "reload(onmt_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_cap = onmt_model_2.models['coco_cap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(str(paths.imgdata_h5_all))\n",
    "\n",
    "def load_vecs(imgids, num_objs=36, feature_dim=2048):\n",
    "    batch_size = len(imgids)\n",
    "    vecs = np.empty((num_objs, batch_size, feature_dim))\n",
    "    for i, idx in enumerate(imgids):\n",
    "        vecs[:, i, :] = f[str(idx)][:]\n",
    "    return Variable(torch.FloatTensor(vecs), volatile=True)\n",
    "\n",
    "\n",
    "def encode_vecs(self, vecs):\n",
    "    # vecs: objs x batch_size x feature_dim\n",
    "    mean_feature = torch.mean(vecs, dim=0)  # batch_size x feature_dim\n",
    "\n",
    "    # Construct the hidden and cell states.\n",
    "    hidden_state = F.tanh(self.init_hidden(mean_feature))\n",
    "    cell_state = F.tanh(self.init_cell(mean_feature))\n",
    "    # hidden_state: batch_size x rnn_size\n",
    "\n",
    "    # To make this look like the output of a sequence RNN, states need to\n",
    "    # have an extra first dimension (per decoder layer) and be packed in a\n",
    "    # tuple.\n",
    "\n",
    "    enc_final = (\n",
    "        hidden_state.unsqueeze(0),\n",
    "        cell_state.unsqueeze(0)\n",
    "    )\n",
    "\n",
    "    return enc_final, vecs\n",
    "\n",
    "vecs = load_vecs(image_set)\n",
    "encoder_final, memory_bank = encode_vecs(coco_cap.model.encoder, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'cat', 'sitting', 'on', 'a']\n"
     ]
    }
   ],
   "source": [
    "def get_logits_multi(model, imgids, tgt_field, tgt_text):\n",
    "    batch_size = len(imgids)\n",
    "\n",
    "    vecs = load_vecs(imgids)\n",
    "    encoder_final, memory_bank = encode_vecs(coco_cap.model.encoder, vecs)\n",
    "\n",
    "    decoder_state = model.decoder.init_decoder_state(vecs, memory_bank=memory_bank, encoder_final=encoder_final)\n",
    "\n",
    "    tgt_vocab = tgt_field.vocab\n",
    "    tokens = [onmt.io.BOS_WORD] + onmt_model_2.tokenize(tgt_text)\n",
    "    print(tokens)\n",
    "    tgt = Variable(torch.LongTensor([tgt_vocab.stoi[tok] for tok in tokens])).unsqueeze(1)\n",
    "    tgt = tgt.expand(-1, len(imgids)).unsqueeze(2)\n",
    "    \n",
    "    dec_out, dec_states, attn = model.decoder(tgt, memory_bank, decoder_state)\n",
    "    logits = model.generator(dec_out).contiguous()\n",
    "    return logits[-1]\n",
    "\n",
    "tgt_field = coco_cap.fields['tgt']\n",
    "all_logits = get_logits_multi(\n",
    "    coco_cap.model,\n",
    "    image_set,\n",
    "    tgt_field,\n",
    "    query_caption\n",
    ").data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25029)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('balcony', -0.4444151222705841),\n",
       " ('window', -2.4897727966308594),\n",
       " ('patio', -3.150484085083008)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onmt_model_2.get_top_k(torch.Tensor(all_logits[0]), tgt_field.vocab.itos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25029)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_bias = logsumexp(all_logits, axis=0, keepdims=True)\n",
    "amped_logits = all_logits - .9 * likelihood_bias\n",
    "#onmt_model_2.get_top_k(torch.Tensor(amped_logits[0]), tgt_field.vocab.itos, 3)\n",
    "\n",
    "likelihood_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000462767.jpg\" style=\"max-width: 200px;\"><div>balcony, window, patio</div><div>balcony, window, patio</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000299933.jpg\" style=\"max-width: 200px;\"><div>dresser, pillow, rocking</div><div>chair, couch, pillow</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000128429.jpg\" style=\"max-width: 200px;\"><div>laptop, screen, lap</div><div>laptop, desk, computer</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000329734.jpg\" style=\"max-width: 200px;\"><div>bench, wooden, picnic</div><div>wooden, bench, red</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000045208.jpg\" style=\"max-width: 200px;\"><div>sink, counter, edge</div><div>counter, table, sink</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000108751.jpg\" style=\"max-width: 200px;\"><div>dining, dinning, table</div><div>table, chair, dining</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000449599.jpg\" style=\"max-width: 200px;\"><div>bookshelf, book, project</div><div>desk, table, bed</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000143123.jpg\" style=\"max-width: 200px;\"><div>tv, fold, television</div><div>chair, wooden, desk</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000580234.jpg\" style=\"max-width: 200px;\"><div>pink, persons, beige</div><div>pink, bed, white</div></div><div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000412749.jpg\" style=\"max-width: 200px;\"><div>couch, pattered, coach</div><div>couch, bed, chair</div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = ''\n",
    "\n",
    "for idx, img_idx in enumerate(image_set):\n",
    "    topks_plain = onmt_model_2.get_top_k(torch.Tensor(all_logits[idx]), tgt_field.vocab.itos, 3)\n",
    "    topks_amp = onmt_model_2.get_top_k(torch.Tensor(amped_logits[idx]), tgt_field.vocab.itos, 3)\n",
    "    url = id2url[id2img[img_idx]['cocoid']]\n",
    "    label_amp = ', '.join(word for word, prob in topks_amp)\n",
    "    html += '<div style=\"display: inline-block;\"><img src=\"{}\" style=\"max-width: 200px;\"><div>{}</div><div>{}</div></div>'.format(\n",
    "        url,\n",
    "        ', '.join(word for word, prob in topks_amp),\n",
    "        ', '.join(word for word, prob in topks_plain))\n",
    "#     print('\\n'.join(f'{prob:0.2f} {word}' for word, prob in topk))    \n",
    "#     print('\\n'.join(f'{prob:0.2f} {word}' for word, prob in topk))\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "-0.44 balcony\n",
      "-2.49 window\n",
      "-3.15 patio\n",
      "-0.00 balcony\n",
      "-0.02 carnival\n",
      "-0.03 gated\n",
      "\n",
      "1\n",
      "-0.60 chair\n",
      "-2.60 couch\n",
      "-2.67 pillow\n",
      "-0.02 watercrafts\n",
      "-0.05 morror\n",
      "-0.06 yarn\n",
      "\n",
      "2\n",
      "-0.67 laptop\n",
      "-2.12 desk\n",
      "-2.77 computer\n",
      "-0.00 online\n",
      "-0.01 midi\n",
      "-0.01 easy-chair\n",
      "\n",
      "3\n",
      "-1.06 wooden\n",
      "-1.75 bench\n",
      "-1.99 red\n",
      "-0.02 pipes\n",
      "-0.02 motorized\n",
      "-0.02 development\n",
      "\n",
      "4\n",
      "-1.69 counter\n",
      "-2.08 table\n",
      "-2.25 sink\n",
      "-0.04 razor\n",
      "-0.04 bucked\n",
      "-0.04 plumber\n",
      "\n",
      "5\n",
      "-0.78 table\n",
      "-1.15 chair\n",
      "-3.23 dining\n",
      "-0.02 steaming\n",
      "-0.06 dinning\n",
      "-0.07 dinnerware\n",
      "\n",
      "6\n",
      "-1.66 desk\n",
      "-2.41 table\n",
      "-2.66 bed\n",
      "-0.00 texter\n",
      "-0.00 project\n",
      "-0.00 crocheting\n",
      "\n",
      "7\n",
      "-0.58 chair\n",
      "-2.73 wooden\n",
      "-2.77 desk\n",
      "-0.03 swiveling\n",
      "-0.03 accommodations\n",
      "-0.06 make-shift\n",
      "\n",
      "8\n",
      "-1.34 pink\n",
      "-2.45 bed\n",
      "-2.80 white\n",
      "-0.01 duffel\n",
      "-0.03 committing\n",
      "-0.04 make-up\n",
      "\n",
      "9\n",
      "-0.57 couch\n",
      "-1.81 bed\n",
      "-2.40 chair\n",
      "-0.02 animal-shaped\n",
      "-0.02 knick-knack\n",
      "-0.02 forsythia\n"
     ]
    }
   ],
   "source": [
    "for tgt_img in range(10):\n",
    "    print()\n",
    "    print(tgt_img)\n",
    "    topk = onmt_model_2.get_top_k(torch.Tensor(all_logits[tgt_img]), tgt_field.vocab.itos, 3)\n",
    "    print('\\n'.join(f'{prob:0.2f} {word}' for word, prob in topk))\n",
    "    \n",
    "    topk = onmt_model_2.get_top_k(torch.Tensor(amped_logits[tgt_img]), tgt_field.vocab.itos, 3)\n",
    "    print('\\n'.join(f'{prob:0.2f} {word}' for word, prob in topk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1920929e-07"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(all_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_bias.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
