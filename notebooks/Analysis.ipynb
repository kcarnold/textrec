{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis pipeline for Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import toolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'get_ipython' in globals():\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    from IPython.display import Image, HTML\n",
    "else:\n",
    "    HTML = lambda *a, **kw: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO captions\n",
      "Loading COCO id2url\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from textrec.paths import paths\n",
    "from textrec import analysis_util, util, notebook_util\n",
    "#reload(analysis_util), reload(util), reload(notebook_util), reload(automated_analyses)\n",
    "from textrec.notebook_util import images, id2img, id2url, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(paths.top_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(show_images([images_by_split['val'][0]['cocoid']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results of writing experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites: Run `textrec.logs_to_csv {batch_name}` and `textrec.gruntwork {batch_name}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = os.environ.get('BATCH', 'spec1')\n",
    "experiment_level_data = pd.read_csv(paths.analyzed / f'experiment_{batch}.csv')\n",
    "block_level_data = pd.read_csv(paths.analyzed / f'block_{batch}.csv')\n",
    "trial_level_data = pd.read_csv(paths.analyzed / f'trial_withmanual_{batch}.csv')\n",
    "helpful_ranks_by_condition = pd.read_csv(paths.analyzed / f'helpful_ranks_by_condition_{batch}.csv').set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefig(fn):\n",
    "    plt.savefig(str(paths.figures / f'{batch}_{fn}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    num_participants=len(set(trial_level_data.participant)),\n",
    "    non_male=(experiment_level_data.gender == 'male').sum().item(),\n",
    "    num_trials_per=trial_level_data.groupby('participant').size().mean().item(),\n",
    "    n_trials=len(trial_level_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general': 15, 'norecs': 6, 'specific': 54}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['helpful_most_votes'] = helpful_ranks_by_condition.loc[:,[col for col in helpful_ranks_by_condition.columns if 'most' in col]].sum(axis=1).to_dict()\n",
    "data['helpful_most_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['helpful_least_votes'] = helpful_ranks_by_condition.loc[:,[col for col in helpful_ranks_by_condition.columns if 'least' in col]].sum(axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Each of 24 participants (0 non-male) completed 9.0 trials per experiment, for a total of 216 trials."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"Each of {num_participants} participants ({non_male} non-male) completed {num_trials_per} trials per experiment, for a total of {n_trials} trials.\".format_map(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Participants spent 29.8 +- 24.7 minutes total."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"Participants spent {total_time_mean:.1f} +- {total_time_std:.1f} minutes total.\".format(\n",
    "    total_time_mean=experiment_level_data.total_time.mean(),\n",
    "    total_time_std=experiment_level_data.total_time.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['participant', 'age', 'english_proficiency', 'gender',\n",
       "       'helpfulRank-accurate-least-condition',\n",
       "       'helpfulRank-accurate-least-idx', 'helpfulRank-accurate-most-condition',\n",
       "       'helpfulRank-accurate-most-idx', 'helpfulRank-quick-least-condition',\n",
       "       'helpfulRank-quick-least-idx', 'helpfulRank-quick-most-condition',\n",
       "       'helpfulRank-quick-most-idx', 'helpfulRank-specific-least-condition',\n",
       "       'helpfulRank-specific-least-idx', 'helpfulRank-specific-most-condition',\n",
       "       'helpfulRank-specific-most-idx', 'other', 'techDiff', 'total_time',\n",
       "       'use_predictive', 'verbalized_during', 'condition_order', 'NFC',\n",
       "       'Extraversion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_level_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'rec_use_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-38de76b06733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                      experiment_level_data.use_predictive.value_counts().to_dict().items()}\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbalized_during'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_level_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbalized_during\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rec_use_group'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_level_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_use_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'used_predictive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'used_predictive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'rec_use_group'"
     ]
    }
   ],
   "source": [
    "data['used_predictive'] = {k.replace(' ', '_'): v for k, v in \n",
    "                                     experiment_level_data.use_predictive.value_counts().to_dict().items()}\n",
    "data['verbalized_during'] = experiment_level_data.verbalized_during.value_counts().to_dict()\n",
    "data['rec_use_group'] = experiment_level_data.rec_use_group.value_counts().to_dict()\n",
    "print('used_predictive', data['used_predictive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "Most participants reported (before beginning the study) that they used predictive typing on their phones:\n",
    "\"\"\" + experiment_level_data.use_predictive.value_counts().to_frame(\"Used predictive typing?\").to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "Several participants answered Yes to \"While you were writing, did you speak or whisper what you were writing?\":\n",
    "\"\"\" + experiment_level_data.verbalized_during.value_counts().to_frame(\"Verbalized?\").to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_level_data.plot.scatter(x='Extraversion', y='NFC');\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1]);\n",
    "savefig('trait_distribution')\n",
    "HTML(\"\"\"\n",
    "We collected 10 items each for NFC and Extraversion.\n",
    "We obtained a wider range of Extraversion than NFC.\n",
    "(Min and max scales normalized to 0 and 1 respectively.)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latexify_conds(txt):\n",
    "    for cond in 'norecs specific general'.split():\n",
    "        txt = txt.replace(cond, f'\\\\S{cond}')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covc = experiment_level_data.condition_order.value_counts()\n",
    "covc.name = 'Num participants'\n",
    "covc = covc.to_latex()\n",
    "covc = covc.replace(',', ', ')\n",
    "covc = latexify_conds(covc)\n",
    "print(covc)\n",
    "data['condition_order_table'] = covc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"When running this experiment, the experiment software was configured to randomize the order\n",
    "of conditions seen by each subject, not to attempt to counterbalance order.\n",
    "This was fixed in subsequent experiments.\"\"\"\n",
    "     + experiment_level_data.condition_order.value_counts().to_frame().to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_means(df, by, outcome):\n",
    "    means = df.groupby(by)[outcome].mean()\n",
    "    data[f'{outcome}_means'] = means.to_dict()\n",
    "    return ', '.join(f'{name}={group_mean:.2f}' for name, group_mean in means.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='TLX_sum', data=block_level_data, capsize=.2)\n",
    "savefig('cogload')\n",
    "HTML(\"The average cognitive load was higher in the no-recommendations condition ({})\".format(\n",
    "    summarize_means(block_level_data, 'condition', 'TLX_sum')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlxen = 'mental physical temporal performance effort frustration'.split()\n",
    "fig, axs = plt.subplots(2, 3, figsize=(10,6), sharey=True)\n",
    "plt.subplots_adjust(wspace=.4, hspace=.3)\n",
    "for i, facet in enumerate(tlxen):\n",
    "    ax = axs.ravel()[i]\n",
    "    sns.barplot(x='condition', y=facet, data=block_level_data, capsize=.2, ax=ax)\n",
    "savefig('tlx_parts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='num_chars', data=trial_level_data, capsize=.2);\n",
    "savefig('num_chars')\n",
    "HTML(\"There was no difference in text length between conditions ({}).\".format(\n",
    "    summarize_means(trial_level_data, 'condition', 'num_chars')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='num_words', data=trial_level_data, capsize=.2);\n",
    "savefig('num_words')\n",
    "summarize_means(trial_level_data, 'condition', 'num_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='all_errors', data=trial_level_data, capsize=.2)\n",
    "HTML(\"Participants made more errors (corrected + uncorrected) in the no-recommendations condition ({}).\".format(\n",
    "    summarize_means(trial_level_data, 'condition', 'all_errors')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='characters_per_sec', data=trial_level_data, capsize=.2)\n",
    "savefig('chars_per_sec')\n",
    "HTML(\"Participants were slightly faster in the recommendations conditions (characters per second: {}).\".format(\n",
    "    summarize_means(trial_level_data, 'condition', 'characters_per_sec')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='rec_use_full_frac', data=trial_level_data, capsize=.2)\n",
    "HTML(\"Participants used available recommendations slightly more often when they were specific (use rate: {}).\".format(\n",
    "    summarize_means(trial_level_data, 'condition', 'rec_use_full_frac')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='orig_efficiency', data=trial_level_data, capsize=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(x for x in experiment_level_data['techDiff'] if x is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(x for x in experiment_level_data['other'] if isinstance(x, str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_level_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fname = paths.analyzed / 'data.yaml'\n",
    "if data_fname.exists():\n",
    "    with open(data_fname, 'r') as f:\n",
    "        yaml_data = yaml.safe_load(f)\n",
    "else:\n",
    "    yaml_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_data[batch] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_fname, 'w') as f:\n",
    "    yaml.safe_dump(yaml_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({batch: data}, open(paths.analyzed / 'data.pkl', 'wb'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_details = pd.read_csv(paths.data / 'num_details_spec1.csv').rename(columns={\n",
    "    'image_id': 'stimulus',\n",
    "    'text': 'corrected_text'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_num_details = pd.merge(\n",
    "    trial_level_data,\n",
    "    num_details,\n",
    "    on=('stimulus', 'corrected_text'),\n",
    "#     right_on=('image_id', 'text'),\n",
    "    how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_num_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='condition', y='num_details', data=with_num_details, capsize=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_num_details.num_details.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(with_num_details.sort_values('num_details').text.iloc[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_num_details.to_csv('with_num_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus the most specific captions for each image. Some participants wrote the most specific captions for several images, so they get mulitple bonuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_num_details.sort_values('num_details', ascending=False).drop_duplicates(['stimulus']).loc[:,['participant', 'corrected_text', 'num_details']].groupby('participant').size() * 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textrec import onmt_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs_cap(cocoid, context, prefix=None):\n",
    "    return onmt_model_2.get_recs('coco_cap', str(cocoid), context, prefix=prefix)\n",
    "\n",
    "def get_recs_lm(cocoid, context, prefix=None):\n",
    "    return onmt_model_2.get_recs('coco_lm', '.', context, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recs_cap(71815, 'a'.split(), prefix='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"a tennis player swinging at a ball\"\n",
    "txt[:0].rsplit(' ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.rindex(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taps_to_type(rec_gen, txt):\n",
    "    actions = []\n",
    "    # Invariant: performing [actions] types txt[:idx]\n",
    "    idx = 0\n",
    "    while idx < len(txt):\n",
    "        sofar = txt[:idx]\n",
    "        if ' ' in sofar:\n",
    "            last_space_idx = sofar.rindex(' ')\n",
    "        else:\n",
    "            last_space_idx = -1\n",
    "        prefix = sofar[:last_space_idx + 1]\n",
    "        cur_word = sofar[last_space_idx + 1:]\n",
    "        cur_desired_word = txt[last_space_idx + 1:].split(' ', 1)[0]\n",
    "#         if cur_desired_word[-1] in ',.;-':\n",
    "#             cur_desired_word = cur_desired_word[:-1]\n",
    "#         print(repr(prefix), repr(cur_word), repr(cur_desired_word))\n",
    "        recs = rec_gen(onmt_model_2.tokenize(prefix), prefix=cur_word)\n",
    "        words = [word for word, rec in recs]\n",
    "        print(prefix, words)\n",
    "        if cur_desired_word in words:\n",
    "            actions.append(dict(type='rec', which=words.index(cur_desired_word), word=cur_desired_word))\n",
    "            idx = last_space_idx + 1 + len(cur_desired_word) + 1\n",
    "        else:\n",
    "            actions.append(dict(type='key', key=txt[idx]))\n",
    "            idx += 1\n",
    "        print(actions[-1])\n",
    "    return actions\n",
    "taps_to_type(partial(get_recs_cap, 71815), \"a young man wearing a red shirt and holding a white and red tennis racket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    writer = pd.ExcelWriter('specificity_details.xlsx')\n",
    "    for stim, group in trial_level_data.groupby('stimulus'):\n",
    "        group.loc[:,['corrected_text']].to_excel(writer, str(stim))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for stim, txts in trial_level_data.groupby('stimulus').text:\n",
    "        print()\n",
    "        print(stim)\n",
    "        print('\\n'.join(txts))\n",
    "        txts.to_csv(f'{stim}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
