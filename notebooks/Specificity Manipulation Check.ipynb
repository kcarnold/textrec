{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import toolz\n",
    "from IPython.display import Image, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<module 'textrec.analysis_util' from '/Users/kcarnold/code/textrec/src/textrec/analysis_util.py'>,\n",
       " <module 'textrec.util' from '/Users/kcarnold/code/textrec/src/textrec/util.py'>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textrec.paths import paths\n",
    "from textrec import analysis_util, util\n",
    "reload(analysis_util), reload(util), reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = util.get_coco_captions()\n",
    "images_by_split = toolz.groupby('split', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2img = {img['cocoid']: img for img in images}\n",
    "id2url = util.get_coco_id2url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(indices, show_captions=True, show_label=True, max_width=200):\n",
    "    def img(idx):\n",
    "        img = id2img[idx]\n",
    "        if show_captions:\n",
    "            captions = '\\n'.join(\n",
    "                '<div>{}</div>'.format(sent)\n",
    "                for sent in toolz.pluck('raw', img['sentences'])\n",
    "            )\n",
    "        else:\n",
    "            captions = ''\n",
    "        label = '<div>{}/{}</div>'.format(img['split'], img['cocoid']) if show_label else ''\n",
    "        return '<div style=\"display: inline-block;\">{}<img src=\"{}\" style=\"max-width: {}px\">{}</div>'.format(\n",
    "            label, id2url[img['cocoid']], max_width, captions)\n",
    "\n",
    "    return '\\n'.join(img(idx) for idx in indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set of similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_vectorizer, caption_vecs = util.get_vectorized_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images(caption, n=10):\n",
    "    query_vec = cap_vectorizer.transform([caption])\n",
    "    similarity = caption_vecs.dot(query_vec.T).A.ravel()\n",
    "    return [images[idx]['cocoid'] for idx in np.argsort(similarity)[-n:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a cat sitting next to a glass bowl, looking up to the camera\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000570528.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000300732.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000400999.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000449770.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000146767.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000548661.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000279386.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000350727.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000002295.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><img src=\"http://images.cocodataset.org/train2017/000000537417.jpg\" style=\"max-width: 200px\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_caption = 'a cat sitting next to a glass bowl, looking up to the camera'\n",
    "print(query_caption)\n",
    "image_set = get_similar_images(query_caption)\n",
    "HTML(show_images(image_set, show_captions=False, show_label=False, max_width=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: Which of the captions is most discriminative for a single one of those images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97da7da8fc848b7bbe01d94fcd261ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('RdBu', 'RdGy', 'PRGn', 'PiYG', 'BrBG', 'RdYlBu', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = sns.choose_colorbrewer_palette('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGQpJREFUeJzt3X/0XHV95/Hni4RfAUTAX4XEEiRW\naPAYyAZsWCQSMFQJW6WKP1p0Cd+zuw3g0tbFQw8VungqpQjHctzNEgVxC42B2kBTQGpoqRUMCIT8\nAAkJK9+m/MaoDQFC3vvHXOw487kzdzJzP+E739fjnDnfO5/5/Jp75/v+3u+de+9bEYGZmeWxy86e\ngJnZeOKga2aWkYOumVlGDrpmZhk56JqZZeSga2aWkYOumVlGDrpmZhk56JqZZTSx7gHm/sU/Vbrk\n7ZQZBybLb75/U1vZY6uebCv7my+cmGw/bf/d28oW3dfeJ8DIUek5VG2fMmfqAW1lKzY+V7nu9C3r\nKo/Vr5cmv6et7NHnX0rW3fWC32krO/irSyq3T62D1PrfffSBZPuU1PzL2qfqvnrjpW1lk44+Kdl+\nyz23t5VtmHNusm7Z9q4q9blIfa7LpLZBqn3Ztkrp5T2lfofL3LHwWFWuXGLqyNLKl9luXHRa3+P1\nynu6ZmYZOeiamWXkoGtmlpGDrplZRg66ZmYZOeiamWXkoGtmlpGDrplZRl0vjpD0LuBU4CAggE3A\nsojId9a+mdmQ6LinK+l/ADcAAn4ArCyWr5d0fv3TMzMbLt32dM8Efj0iXmkulHQ5sAb401QjSSPA\nCMC7Tv9DDpo9fwBTNTMb+7od090OpG5I8CvFa0kRsSgiZkbETAdcM7N/121P97PA30t6FHiiKHs7\ncCiwsM6JmZkNo45BNyJulfROYBaNL9IEjAIrI+LVDPMzMxsqXc9eiIjtwN0Z5mJmNvR8nq6ZWUYO\numZmGTnompll5KBrZpaRg66ZWUYOumZmGdWeDfjyDx/RVlY1E2ypRObg8uyk7ZlUy6Sy/KYymabe\nU+c5/LJUdtcyqayzvWSo7cWkZOFhybpT5s7sa6zUOkhl7i17T2XroGr7CYlswL1IjV+WoXfa/u2f\n114y76b6Ta2rVIbjMsksySXb+rybHmorS/0OpOpBeabv8cp7umZmGTnompllVPvhBTOznN7x7rft\n7Cl05D1dM7OMHHTNzDJy0DUzy8hB18wsIwddM7OMHHTNzDLa4aAr6TODnIiZ2XjQz57uRWUvSBqR\ndK+ke5d+85o+hjAzGy4dL46QtKrsJeCtZe0iYhGwCGDVps2xw7MzM9uJJM0DrgQmAFdHxJ+2vP52\n4FrgjUWd8yNieac+u12R9lbgA8ALrXMB/rn61M3MxhZJE4CrgBMpEvJKWhYRa5uq/RGwJCK+Kulw\nYDlwcKd+uwXdW4C9I6LtlkSS7qw+fTOzMWcWsD4iNgBIugE4FWgOugG8oVjeF2i/LWGLbinYz+zw\n2ie6dW5m9nomaQQYaSpaVBweBTgIeKLptVHg6JYuvgDcLulsYC9gbrcxfcMbMxu3mr9/SlCqScvz\njwPXRMSfS3ovcJ2k6RGxvWxMn6drZpY2Ckxpej6Z9sMHZwJLACLi+8AewJs6deqga2aWthKYJmmq\npN2A04FlLXV+DJwAIOkwGkH3mU6dOuiamSVExDZgIXAbsI7GWQprJF0saX5R7feBsyQ9CFwPfDoi\nOp4m62O6ZmYlinNul7eUXdi0vBaY3UuftQfddLK+6okZUwkrUwkkoXrCx7IkmKlkfSML2hMwHn7O\nLZXGAbiics20h97XX/uqCRyhJLFhDwkUU8qSNSa34dT2xIjTW78r7lHp+08lweyh30cu+WJb2cFf\nXVK5fWq9lCWrTJVPS2yrZLLJkrqpNzt9y7pk+5sTvwO9JNZMJXctc+7sQyrXHauG5vBCLxl2zcx2\nlqEJumZmY4GDrplZRg66ZmYZOeiamWXkU8bMbKicMiN9dtLrhfd0zcwyctA1M8uoa9CV9C5JJ0ja\nu6V8Xn3TMjMbTh2DrqRzgL8BzgZWSzq16eX2S3LMzKyjbl+knQUcFRE/l3QwsFTSwRFxJel7TZqZ\nWQfdDi9MiIifA0TE48DxwMmSLqdD0G3OBrx48eJBzdXMbMzrtqf7pKT3vJYjrdjj/RDwNeCIskbN\nd2N/cetWZwM2Myt029P9XeDJ5oKI2BYRvwscV9uszMyGVLfElKMdXvve4KdjZjbcfJ6umVlGDrpm\nZhk56JqZZeSga2aWkYOumVlGDrpmZhmpS4r2vm1ff3fbAKmss6/eeGmyfSqba7/ty9zyHxe0lb3/\nso+2lU34yOcq95nK0JrMultSd/Wk9gy5KzY+l2zfS9bVlG/v9922sl7WX8qWe25PlqfWYWoblo2f\n6rffzMdln6Gq+l1XvXwuytZrSmpe/bZP6WVbl9lzjz36vr3Ald/bUDmonTv7kOy3M/CerplZRg66\nZmYZOeiamWXkoGtmlpGDrplZRg66ZmYZOeiamWXU7SbmZmZjypypB+zsKXTUNehKmgVERKyUdDgw\nD3g4IpbXPjszsyHTMehK+mPgZGCipO8ARwN3AudLmhERl9Q/RTOz4dHtmO5pwGwaqXl+D/hPEXEx\n8AHgY2WNmhNTLrrh2wObrJnZWNft8MK2iHgV2CLpsYj4KUBEvChpe1mj5sSUqXsvmJmNV932dF+W\nNKlYPuq1Qkn7AqVB18zM0roF3eMiYgtARDQH2V2BM2qblZnZ64CkeZIekbRe0vkldT4qaa2kNZL+\nsluf3bIBv1RS/izwbKVZm5mNQZImAFcBJwKjwEpJyyJibVOdacDngdkR8YKkt3Tr1xdHmJmlzQLW\nR8SGiHgZuAE4taXOWcBVEfECQEQ83a1TB10zs7SDgCeano8WZc3eCbxT0vck3S1pXrdOfUWamY1b\nkkaAkaaiRcXZVwCprBKtZ2NNBKYBxwOTgbskTY+In5SN6aBrZuNW8+mtCaPAlKbnk4HWnFijwN0R\n8QqwUdIjNILwyrIxfXjBzCxtJTBN0lRJuwGnA8ta6nwbmAMg6U00Djds6NSpg66ZWUJEbAMWArcB\n64AlEbFG0sWS5hfVbgOek7QWWAH8YUSkM8cWas8G/OLWrX0N8Ojz7WetpbLhjhx1YLJ9KsNrWXbS\nqtlov/JU+qyQ1N2Npu2/e1tZ6j2V1U1lgi3Tb4bck27d1lZ2yoz0ek291/NueqjyWJd/+Ii2sn7f\nf79Zoh+55IttZVPmzqw8fi9Zb8s+A1VN37Kuct3UeuklS3W/enmv7z5w376z867atLlyzBnEeL3y\nnq6ZWUYOumZmGTnompll5KBrZpaRg66ZWUYOumZmGTnompll5KBrZpZRz0FX0jfqmIiZ2XjQLRtw\n63XGAuZIeiNARMxvb2VmtvOkrmx8Pem2pzsZ+ClwOfDnxeNnTctJzdmAFy9ePKi5mpmNed1u7TgT\nOBe4gMaNHB6Q9GJE/EOnRs23S+v33gtmZsOkW4607cCXJX2r+PlUtzZmZlauUgCNiFHgtyV9kMbh\nBjMz2wE97bVGxN8Cf1vTXMzMhp7P0zUzy8hB18wsIwddM7OMHHTNzDJy0DUzy8hB18wso9ovdKia\nCbSu66U3zDm3faxaRkpnKYb2rLnpeum603sYv5fMvyk3L6ie+TaVTbaX9qdcfW97WSLz8Jyph1Xu\nc8V9m9rKRkoy9KY+lanMv2XrNJV5uRf9Zol+af9qGX57UdY+lSV4UWJdpzJEQ29Zou9YeGzlumOV\n93TNzDJy0DUzy8hB18wsIwddM7OMHHTNzDJy0DUzy8hB18wsIwddM7OMero4QtKxwCxgdUT0d3a4\nmdk41HFPV9IPmpbPAv4C2Af4Y0nn1zw3M7Oh0+3wwq5NyyPAiRFxEXAS8MmyRs3ZgJd+85r+Z2lm\nNiS6HV7YRdJ+NIKzIuIZgIj4N0nbyho1ZwNetWmzswGbmRW6Bd19gfsAASHpbRHxpKS9izIzs9eV\nnm78c+gx9U2kRLcU7AeXvLQd+K2Bz8bMbMjt0CljEbElIjYOejJmZq8nkuZJekTS+k4nD0g6TVJI\n6np/U5+na2aWIGkCcBVwMnA48HFJhyfq7QOcA9xTpV8HXTOztFnA+ojYEBEvAzcApybq/QlwKbC1\nSqcOumZmaQcBTzQ9Hy3KfkHSDGBKRNxStVMHXTMbt5qvKSgeI80vJ5pEU9tdgC8Dv9/LmLXnSDMz\ne71qvqYgYRSY0vR8MtCcHG4fGmkM75QE8DZgmaT5EdGeBLCgiHqvXXhx69a2AVLn0ZUl+usl2eLq\nSe1JDFNJ/V698dJk++/+wZK2svdf9tG+5pRTah32Mtd+ky32IjWvOuafSkwKnZKD/rKb729PwAjp\nJJpnv/XpyvNK6fe95tzWvYyVSmxZZs899uj7/P/t6++uHNR2OfSY0vEkTQR+BJwA/AuwEvhERKwp\nqX8n8AedAi4M0eGFVMA1M9tREbENWAjcBqwDlkTEGkkXS5q/o/368IKZWYmIWA4sbym7sKTu8VX6\nHJo9XTOzscBB18wsIwddM7OMHHTNzDJy0DUzy8hB18wso2450o6W9IZieU9JF0m6WdKXJO2bZ4pm\nZsOj257u14AtxfKVNDJJfKko+3qN8zIzG0pdc6QVV2UAzIyII4vlf5LUQ04MMzOD7nu6qyV9plh+\n8LW7okt6J/BKWaPmO/csXrx4QFM1Mxv7uu3pLgCulPRHwLPA9yU9QeMekwvKGjXfuSd1wxszs/Gq\nW2LKzcCni3QUhxT1RyPiqRyTMzMbNpVueBMRPwMerHkuZmZDz3cZM7Oh0su9gvc+9JgaZ5LmiyPM\nzDJy0DUzy8hB18wsIwddM7OMHHTNzDKq/eyFR59/qb0wlbX3I+mMoV+5rz0baypD680L2rP+Qjrz\n8OqSDLHvv6y9bMJHPldpTgBzph7QVpbKRpxcJyV1U1LvaRBSGV7LEn5O37Kuct2qph9dvW7qG+rU\ntppesq4Oeby9/RN3tCdxPXPuzGT7SW9tX1cn3botURNOmfGpZHmrKy79YbL8He9+W1vZzQva32v6\nUwWLEp/XkcS6KlP2ea3qvKs7Jsf9JXcsPLavscYC7+mamWXkoGtmlpGDrplZRg66ZmYZOeiamWXk\noGtmlpGDrplZRg66ZmYZdcsGfI6kKbkmY2Y27Lrt6f4JcI+kuyT9N0lvzjEpM7Nh1S3obgAm0wi+\nRwFrJd0q6YwihU9Sc2LKpd+8ZnCzNTMb47rdeyEiYjtwO3C7pF2Bk4GPA5cByT3f5sSUqzZtdmJK\nM7NCt6Cr5icR8QqwDFgmac/aZmVmNqS6HV74WNkLEfHigOdiZjb0OgbdiPhRromYmY0HPk/XzCwj\nB10zsxKS5kl6RNJ6SecnXj9P0lpJqyT9vaRf7dZn7ZkjzMxySmUAKXPYJ8tfkzQBuAo4ERgFVkpa\nFhFrm6rdD8yMiC2S/itwKR2+CwPv6ZqZlZkFrI+IDRHxMnADcGpzhYhYERFbiqd307iuoSMHXTOz\ntIOAJ5qejxZlZc4E/q5bpz68YGbjlqQRYKSpaFFxcRe0XKdQSF7sJelTwEzgfd3GrD3opjLcprKT\nTtv/wMp9njKjet1epLLhksgmO2dqf1lve/HqjZe2lW1J1CuTyppbJvn+e+i3LKNzStUMs73MP+Wl\nySVz6rPflMs/fESyvGqW51Q26TKp36FUlmxI/76k1v95Nz2UbH/zgvaMyL1kCK7r93UQmq+eTRgF\nmm/4NRloW8mS5gIXAO+LiK4rxocXzMzSVgLTJE2VtBtwOo0rcn9B0gzgfwPzI+LpKp066JqZJUTE\nNmAhcBuwDlgSEWskXSxpflHtz4C9gW9JekDSspLufsHHdM3MSkTEcmB5S9mFTctze+3Te7pmZhk5\n6JqZZeSga2aWkYOumVlGDrpmZhl1PHuh6dy0TRFxh6RPAL9B4/SJRUUmCTMzq6jbKWNfL+pMknQG\njfPRbgJOoHEziDPqnZ6Z2XDpdnjhiIj4GPBbwEnAaRFxHfAZYEZZo+ZswIsXLx7cbM3Mxrhue7q7\nFIcY9gImAfsCzwO7A7uWNWq+nvnFrVudDdjMrNAt6C4GHgYm0Lihw7ckbQCOoXFvSTMz60HHoBsR\nX5b0V8XyJknfAOYC/ycifpBjgmZmw6TrvRciYlPT8k+ApbXOyMxsiPk8XTOzjBx0zcwyctA1M8vI\nQdfMLCPfxNzMhsqjt26oXDdftsN/5z1dM7OMat/TrZo1tJfsoldc+8O2srJMqiueekuibuWhWD2p\n/W9hWdbUlF4yoa7Y2F52dp8ZenvJ8JvMnNvDdkkp264rNj7XVjbtqPbxJ5Rk8909kaW5l5mm1suU\nivUAHrnki+2Fl1yXrJtaB6n3Xyb12R45qv1zlSrrRSrrL8ApV99bqf1jq57sa3yAc2cf0ncfr3fe\n0zUzy8hB18wsIwddM7OMHHTNzDJy0DUzy8hB18wsIwddM7OMHHTNzDLqenGEpHfQyJE2BdgGPApc\nHxGba56bmdnQ6binK+kc4H8BewD/AdiTRvD9vqTja5+dmdmQ6XZ44SxgXkT8Txppeg6PiAuAecCX\nyxo1ZwNe+s1rBjZZM7Oxrsq9FyYCr9LIALwPQET8WFKlbMCrNm12NmAzs0K3oHs1sFLS3cBxwJcA\nJL2ZRip2MzPrQbdswFdKuoPGbScvj4iHi/JnaARhMzPrQZVswGuANRnmYmY29HyerplZRg66ZmYZ\nOeiamZWQNE/SI5LWSzo/8frukv6qeP0eSQd369NB18wsQdIE4CrgZOBw4OOSDm+pdibwQkQcSuPa\nhS9169dB18wsbRawPiI2RMTLwA3AqS11TgWuLZaXAidIUqdOHXTNzNIOAp5oej5alCXrRMQ2YDOQ\nzpL7mojI9gBGqpT1Unestx9Lc93Z7cfSXHd2+7E017L2OR7ACHBv02Ok6bXfBq5uev47wFda2q8B\nJjc9fww4oOOYmd/gvVXKeqk71tuPpbnu7PZjaa47u/1YmmtZ+539AN4L3Nb0/PPA51vq3Aa8t1ie\nCDwLqFO/PrxgZpa2Epgmaaqk3YDTgWUtdZYBZxTLpwHfjSICl6lywxszs3EnIrZJWkhjb3YC8LWI\nWCPpYhp758uAxcB1ktbTuB/N6d36zR10F1Us66XuWG+fc6yx3j7nWGO9fc6x6mq/00XEcmB5S9mF\nTctbaRz7rUxd9oTNzGyAfEzXzCyjgQfdbpfNFXXOkPRo8TgjVafiWF+T9LSk1f3Mp8I4UyStkLRO\n0hpJ59Y41h6SfiDpwWKsi0rqDWodTpB0v6Rbah7ncUkPSXpA0r0ldfpef0U/b5S0VNLDxTZ7bx1j\nSfq14v289vippM/WMVbRz38vPhOrJV0vaY9Enb63l6RzizHWpN5PUWcg72lcGvApFhNonKd2CLAb\n8CCNFD/NdfYHNhQ/9yuW99vB8Y4DjgRW7+h8Ko7zK8CRxfI+wI8S72tQYwnYu1jeFbgHOKbGdXge\n8JfALYnXBjnO48Cb+vns9DDWtcCCYnk34I11jdXS55PAr9b0uTgI2AjsWTxfAnx60NsLmA6sBibR\n+M7nDmBa3etvPD0Gvadb5bK5DwDfiYjnI+IF4Ds0cq71LCL+kc4ZLKrMp8o4/xoRPyyWfwaso/3K\nlEGNFRHx8+LprsWj9cD7QNahpMnAB2lkCEkZ2LaqYCDrT9IbaPwxXgwQES9HxE/qGKvFCcBjEfH/\nahxrIrCnpIk0guKmltcHsb0OA+6OiC3RuMLqH2hkA29Wx/obNwYddHu6bK5DnZzz6UlxF6EZNPZA\naxmr+Jf/AeBpGr9EdY11BfA5YHvJ64NcfwHcLuk+SSM1jnUI8Azw9eKwydWS9qpprGanA9cnygcy\nVkT8C3AZ8GPgX4HNEXF7DWOtBo6TdICkScBv0sgAPuhxxq1BB93UjR5a99Kq1BmUgY4laW/gRuCz\nEfHTusaKiFcj4j3AZGCWpOmDHkvSh4CnI+K+TtX6HafJ7Ig4ksYdm35PUmu6p0GNNZHGIaevRsQM\n4N+A1mOOg/5c7AbMB76VenkQY0naj8be5FTgQGAvSZ8a9FgRsY7GnbK+A9xK49DBtkGPM54NOuiO\n8st/FScDzzZ90TC/pE7rv0k7pPjC67Wx/ssgx1Ij+/GNwP+NiJvqHOs1xb/FdwIfrGEdzgbmS3qc\nxr+H75f0d3Vtq4jYVPx8Gvhr4H01rb9RYLTpv4OlwG/UvK1OBn4YEU/V+LmYC2yMiGci4hXgJuCT\ndWyviFgcEUdGxHE0Dt/9pO7P+rgyyAPENPYyNtD4a/zaAfZfb6mzP40vBPYrHhuB/fsY82DKv0jr\nOp+KYwj4BnBFP++94lhvpvjiB9gTuAv4UM3r8HjKv0jrexxgL2CfpuV/BubVsf6Kvu4Cfq1Y/gLw\nZ3WNVfR3A/CZmj8XR9O4ucqk4vN4LXB2TdvrLcXPtwMP0/Jl3KDX33h7DL7DxjGgH9H4dvOCkjr/\nGVhfPJIf1opjXU/j+NYrNP76nrkj86kwzrE0/n1aBTxQPH6zprHeDdxfjLUauLDOdVj0lQy6gxqH\nxnHWB4vHmg6fi77XX9HPe2jcMWoV8O3WoDHgsSYBzwH7dqgzqLEuKoLgauA6YPeattddwNpie51Q\n53sajw9fkWZmlpGvSDMzy8hB18wsIwddM7OMHHTNzDJy0DUzy8hB18wsIwddM7OMHHTNzDL6/67L\n2VhIKLtVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x161a90080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_vectors = cap_vectorizer.transform([util.join_captions(id2img[id]) for id in image_set])\n",
    "individual_cap_vectors = cap_vectorizer.transform([' '.join(sent['tokens']) for id in image_set for sent in id2img[id]['sentences']])\n",
    "cap_srcs_txt = [f'{i}-{j}' if j == 0 else '' for i, id in enumerate(image_set) for j, sent in enumerate(id2img[id]['sentences'])]\n",
    "cap_srcs = [(i,j) for i, id in enumerate(image_set) for j, sent in enumerate(id2img[id]['sentences'])]\n",
    "similarities = image_vectors.dot(individual_cap_vectors.T).A\n",
    "sns.heatmap(similarities, cmap=cmap, xticklabels=cap_srcs_txt);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat sitting on top of the table beside a dish. \n",
      "an orange and white cat laying on a brown table\n",
      "A small cat sitting under a wooden table.\n",
      "A black and white cat sitting on top of a table.\n",
      "a close up of a cat near a bowl\n",
      "A cat is looking at an empty bowl on the floor.\n",
      "A cat sitting in a bowl on a table looking at the camera.\n",
      "a cat sitting on a table next to a colorful bowl\n",
      " A cat sitting on the floor near a bag.\n",
      "A cat sitting in front of his food bowl looking at the camera.\n",
      "A brown and white cat sitting next to a glass.\n",
      "A golden colored cat sitting next to a bowl.\n",
      "A cat staring at the bottom of a brown bowl. \n",
      "A gray cat laying on the floor next to a food dish.\n",
      "A grey cat sitting in a blue bowl\n",
      "A cat peering into a wooden bowl which is sitting on a table.\n",
      "a brown cat sitting inside a fancy bowl\n",
      "A ginger cat lying on a table next to a bowl\n",
      "A large orange cat sitting in a white glass bowl.\n",
      "A black cat is standing next to a bowl on a table\n",
      "There is a adult cat that is sitting in a bowl\n",
      "A cat is sitting next to a camera in a case.\n",
      "A black cat standing over a glass bowl.\n",
      "A cat sits on the table next to a bowl.\n",
      "A cat is sitting in a bowl on a table.\n",
      "A cat sitting inside of a bowl on top of  a table.\n",
      "A cat leaned over looking at a glass on the floor.\n",
      "A cat looking back is sitting in a bowl.\n",
      "A cat is sitting in a white bowl.\n",
      "A black and white cat is sitting in a ceramic bowl.\n",
      "A cat sitting in a bowl on a table \n",
      "a cat sitting on a table next to a bowl of fruit\n",
      "Cat laying down with a bowl in front of it.\n",
      "The cat sits in the bowl on the floor.\n",
      "A cat is sitting in a bowl which is on the floor.\n",
      "A cat looking inside a wooden bowl for some food.\n",
      "A grey cat is laying next to a metal bowl.\n",
      "The cat is sitting in the bowl on the floor. \n",
      "A cat laying next to a stainless steel bowl.\n",
      "A black and white cat eating from a ceramic bowl.\n",
      "a cat that is sitting next to a backpack\n",
      "A cat that is sitting next to a backpack.\n",
      "A cat sitting next to an open camera bag.\n",
      "a cat underneath a wood table playing with a glass cup\n",
      "A cat is playing with a glass under a table.\n",
      "A CAT ON A WOODEN SURFACE IS LOOKING AT A WOODEN BOWL\n",
      "A black and white cat sits in a bowl on a table.\n",
      "A black cat sits on a small white table next to a fruit bowl.\n",
      "A black cat sits on a table next to a fruit bowl.\n",
      "A black and white defensively looking up by its bowl.\n"
     ]
    }
   ],
   "source": [
    "dynamic_range = np.max(similarities, axis=0) - np.min(similarities, axis=0)\n",
    "for k in np.argsort(dynamic_range):\n",
    "    i, j= cap_srcs[k]\n",
    "    print(id2img[image_set[i]]['sentences'][j]['raw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back on track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_pairs = json.load(open(paths.data / 'stimulusPairs.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440500,http://images.cocodataset.org/train2017/000000440500.jpg\n"
     ]
    }
   ],
   "source": [
    "stim_idx = 11\n",
    "print('{},{}'.format(stim_pairs[stim_idx][1]['id'], stim_pairs[stim_idx][1]['url']))\n",
    "\n",
    "image_set = get_similar_images(util.join_captions(id2img[stim_pairs[stim_idx][0]['id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: inline-block;\"><div>val/280480</div><img src=\"http://images.cocodataset.org/train2017/000000280480.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/459563</div><img src=\"http://images.cocodataset.org/train2017/000000459563.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/108982</div><img src=\"http://images.cocodataset.org/train2017/000000108982.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/234834</div><img src=\"http://images.cocodataset.org/train2017/000000234834.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/306421</div><img src=\"http://images.cocodataset.org/train2017/000000306421.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/31396</div><img src=\"http://images.cocodataset.org/train2017/000000031396.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/176904</div><img src=\"http://images.cocodataset.org/train2017/000000176904.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/440706</div><img src=\"http://images.cocodataset.org/train2017/000000440706.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>test/574256</div><img src=\"http://images.cocodataset.org/train2017/000000574256.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/128791</div><img src=\"http://images.cocodataset.org/train2017/000000128791.jpg\" style=\"max-width: 200px\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(show_images(image_set, show_captions=False, show_label=True, max_width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = np.random.choice(len(image_set))\n",
    "target = image_set[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cap(target, n_words=5):\n",
    "    cap = []\n",
    "    for i in range(n_words):\n",
    "        recs = onmt_model_2.get_recs('coco_cap', str(target), cap)\n",
    "        cap.append(recs[0])\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a small bird perched on\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(gen_cap(target, n_words=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a small bird perched on a branch of a tree\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(gen_cap(target, n_words=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess? 280480\n",
      "guess? 128791\n",
      "guess? 0\n",
      "guess? 0\n",
      "guess? 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "':('"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_guess():\n",
    "    for i in range(5):\n",
    "        guess = int(input('guess? '))\n",
    "        if guess == target:\n",
    "            return i + 1\n",
    "    return ':('\n",
    "do_guess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234834"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prob of each image in target set given caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_cap = onmt_model_2.models['coco_cap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdata_h5_all = paths.top_level / 'models-aside' / 'feats_by_imgid.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.imgdata_h5 = imgdata_h5_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(str(imgdata_h5_all))\n",
    "\n",
    "def load_vecs(imgids, num_objs=36, feature_dim=2048):\n",
    "    batch_size = len(imgids)\n",
    "    vecs = np.empty((num_objs, batch_size, feature_dim))\n",
    "    for i, idx in enumerate(imgids):\n",
    "        vecs[:, i, :] = f[str(idx)][:]\n",
    "    return Variable(torch.FloatTensor(vecs), volatile=True)\n",
    "\n",
    "\n",
    "def encode_vecs(self, vecs):\n",
    "    # vecs: objs x batch_size x feature_dim\n",
    "    mean_feature = torch.mean(vecs, dim=0)  # batch_size x feature_dim\n",
    "\n",
    "    # Construct the hidden and cell states.\n",
    "    hidden_state = F.tanh(self.init_hidden(mean_feature))\n",
    "    cell_state = F.tanh(self.init_cell(mean_feature))\n",
    "    # hidden_state: batch_size x rnn_size\n",
    "\n",
    "    # To make this look like the output of a sequence RNN, states need to\n",
    "    # have an extra first dimension (per decoder layer) and be packed in a\n",
    "    # tuple.\n",
    "\n",
    "    enc_final = (\n",
    "        hidden_state.unsqueeze(0),\n",
    "        cell_state.unsqueeze(0)\n",
    "    )\n",
    "\n",
    "    return enc_final, vecs\n",
    "\n",
    "vecs = load_vecs(image_set)\n",
    "encoder_final, memory_bank = encode_vecs(coco_cap.model.encoder, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'cat',\n",
       " 'sitting',\n",
       " 'next',\n",
       " 'to',\n",
       " 'a',\n",
       " 'glass',\n",
       " 'bowl,',\n",
       " 'looking',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'camera']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_cap.fields['tgt'].preprocess(query_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.251428 , 4.11986  , 4.104523 , 4.046769 , 3.904194 , 3.8581712,\n",
       "       3.8271136, 3.976628 , 4.283532 , 4.273225 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_logprobs_varying_image(model, imgids, tgt_field, tgt_text):\n",
    "    batch_size = len(imgids)\n",
    "\n",
    "    vecs = load_vecs(image_set)\n",
    "    encoder_final, memory_bank = encode_vecs(coco_cap.model.encoder, vecs)\n",
    "\n",
    "    decoder_state = model.decoder.init_decoder_state(vecs, memory_bank=memory_bank, encoder_final=encoder_final)\n",
    "#    decoder_state.repeat_beam_size_times(batch_size)\n",
    "#    memory_bank = memory_bank.repeat(1, batch_size, 1)\n",
    "\n",
    "    # \"process\" handles padding and numericalization\n",
    "    tgt = tgt_field.process([tgt_text] * batch_size, device=-1, train=False)\n",
    "    pad_idx = tgt_field.vocab.stoi[tgt_field.pad_token]\n",
    "    \n",
    "    # Decoder wants an extra dim for extra features.\n",
    "    dec_out, dec_states, attn = model.decoder(tgt[:-1].unsqueeze(2), memory_bank, decoder_state)\n",
    "    logits = model.generator(dec_out).contiguous()\n",
    "    seq_len, batch_size_2, num_vocab = logits.shape\n",
    "    assert batch_size == batch_size_2\n",
    "    losses = F.nll_loss(logits.view(seq_len * batch_size, num_vocab), tgt[1:].view(seq_len * batch_size), reduce=False)\n",
    "    mask = tgt[1:].eq(pad_idx)\n",
    "    losses = losses.view(seq_len, batch_size).masked_fill(mask, 0).data.sum(0)\n",
    "    length = (~mask.data).long().sum(0)\n",
    "    return losses / length.float()\n",
    "\n",
    "losses_by_img = eval_logprobs_varying_image(\n",
    "    coco_cap.model,\n",
    "    image_set,\n",
    "    coco_cap.fields['tgt'],\n",
    "    coco_cap.fields['tgt'].preprocess(query_caption)\n",
    ").numpy()\n",
    "losses_by_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(losses_by_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: inline-block;\"><div>val/279386</div><img src=\"http://images.cocodataset.org/train2017/000000279386.jpg\"><div>A small cat sitting under a wooden table.</div>\n",
       "<div>A brown and white cat sitting next to a glass.</div>\n",
       "<div>a cat underneath a wood table playing with a glass cup</div>\n",
       "<div>A cat is playing with a glass under a table.</div>\n",
       "<div>A cat leaned over looking at a glass on the floor.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>train/548661</div><img src=\"http://images.cocodataset.org/train2017/000000548661.jpg\"><div>a cat sitting on a table next to a colorful bowl</div>\n",
       "<div>A ginger cat lying on a table next to a bowl</div>\n",
       "<div>A cat sitting on top of the table beside a dish. </div>\n",
       "<div>an orange and white cat laying on a brown table</div>\n",
       "<div>A golden colored cat sitting next to a bowl.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>test/146767</div><img src=\"http://images.cocodataset.org/train2017/000000146767.jpg\"><div>A cat laying next to a stainless steel bowl.</div>\n",
       "<div>a close up of a cat near a bowl</div>\n",
       "<div>A gray cat laying on the floor next to a food dish.</div>\n",
       "<div>A grey cat is laying next to a metal bowl.</div>\n",
       "<div>Cat laying down with a bowl in front of it.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>train/350727</div><img src=\"http://images.cocodataset.org/train2017/000000350727.jpg\"><div>A cat is sitting in a bowl which is on the floor.</div>\n",
       "<div>There is a adult cat that is sitting in a bowl</div>\n",
       "<div>A grey cat sitting in a blue bowl</div>\n",
       "<div>The cat is sitting in the bowl on the floor. </div>\n",
       "<div>The cat sits in the bowl on the floor.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>train/449770</div><img src=\"http://images.cocodataset.org/train2017/000000449770.jpg\"><div> A cat sitting on the floor near a bag.</div>\n",
       "<div>A cat is sitting next to a camera in a case.</div>\n",
       "<div>A cat sitting next to an open camera bag.</div>\n",
       "<div>a cat that is sitting next to a backpack</div>\n",
       "<div>A cat that is sitting next to a backpack.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>train/400999</div><img src=\"http://images.cocodataset.org/train2017/000000400999.jpg\"><div>A cat sits on the table next to a bowl.</div>\n",
       "<div>A black cat is standing next to a bowl on a table</div>\n",
       "<div>A black cat sits on a small white table next to a fruit bowl.</div>\n",
       "<div>a cat sitting on a table next to a bowl of fruit</div>\n",
       "<div>A black cat sits on a table next to a fruit bowl.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>val/300732</div><img src=\"http://images.cocodataset.org/train2017/000000300732.jpg\"><div>A black cat standing over a glass bowl.</div>\n",
       "<div>A black and white cat sitting on top of a table.</div>\n",
       "<div>A cat sitting in front of his food bowl looking at the camera.</div>\n",
       "<div>A black and white defensively looking up by its bowl.</div>\n",
       "<div>A black and white cat eating from a ceramic bowl.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/570528</div><img src=\"http://images.cocodataset.org/train2017/000000570528.jpg\"><div>A large orange cat sitting in a white glass bowl.</div>\n",
       "<div>A cat sitting in a bowl on a table looking at the camera.</div>\n",
       "<div>a brown cat sitting inside a fancy bowl</div>\n",
       "<div>A cat looking back is sitting in a bowl.</div>\n",
       "<div>A cat is sitting in a white bowl.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>train/537417</div><img src=\"http://images.cocodataset.org/train2017/000000537417.jpg\"><div>A black and white cat is sitting in a ceramic bowl.</div>\n",
       "<div>A cat sitting inside of a bowl on top of  a table.</div>\n",
       "<div>A black and white cat sits in a bowl on a table.</div>\n",
       "<div>A cat sitting in a bowl on a table </div>\n",
       "<div>A cat is sitting in a bowl on a table.</div></div>\n",
       "<div style=\"display: inline-block;\"><div>test/2295</div><img src=\"http://images.cocodataset.org/train2017/000000002295.jpg\"><div>A cat peering into a wooden bowl which is sitting on a table.</div>\n",
       "<div>A CAT ON A WOODEN SURFACE IS LOOKING AT A WOODEN BOWL</div>\n",
       "<div>A cat is looking at an empty bowl on the floor.</div>\n",
       "<div>A cat staring at the bottom of a brown bowl. </div>\n",
       "<div>A cat looking inside a wooden bowl for some food.</div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(show_images([image_set[idx] for idx in np.argsort(losses_by_img)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do more specific captions get scored higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301595,http://images.cocodataset.org/train2017/000000301595.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: inline-block;\"><div>val/396295</div><img src=\"http://images.cocodataset.org/train2017/000000396295.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>val/301595</div><img src=\"http://images.cocodataset.org/train2017/000000301595.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/574147</div><img src=\"http://images.cocodataset.org/train2017/000000574147.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/102644</div><img src=\"http://images.cocodataset.org/val2017/000000102644.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/316766</div><img src=\"http://images.cocodataset.org/train2017/000000316766.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/448613</div><img src=\"http://images.cocodataset.org/train2017/000000448613.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/259733</div><img src=\"http://images.cocodataset.org/train2017/000000259733.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>restval/362026</div><img src=\"http://images.cocodataset.org/train2017/000000362026.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>test/445812</div><img src=\"http://images.cocodataset.org/train2017/000000445812.jpg\" style=\"max-width: 200px\"></div>\n",
       "<div style=\"display: inline-block;\"><div>train/480239</div><img src=\"http://images.cocodataset.org/train2017/000000480239.jpg\" style=\"max-width: 200px\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stim_idx = 1\n",
    "print('{},{}'.format(stim_pairs[stim_idx][1]['id'], stim_pairs[stim_idx][1]['url']))\n",
    "image_set = get_similar_images(util.join_captions(id2img[stim_pairs[stim_idx][0]['id']]))\n",
    "HTML(show_images(image_set, show_captions=False, show_label=True, max_width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_caption = 'a bathroom with a toilet'\n",
    "specific_captions = [\n",
    "    'a brown towel hanging on', 'a brown towel hangs on',\n",
    "    'stone sink counter glass shower', 'a modern bathroom with stone',\n",
    "    'black and white tiled shower', 'a shower with black and',\n",
    "    'ledge behind open toilet seat', 'a toilet with lid up',\n",
    "    'open sliding shower behind toilet', 'a bathroom with beige floor',\n",
    "    'adjustable shower head in corner', 'a bathroom with cabinets under',\n",
    "    'spacious white symmetrical shower glass', 'a large bright white shower',\n",
    "    'towel hanger over radiator beside', 'a dim bathroom with small',\n",
    "    'green towel on shower door', 'two framed photos hang above',\n",
    "    'white towel on shower door', 'a bathroom with a closed' #(shower)\n",
    "]\n",
    "candidate_captions = specific_captions + [generic_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_logprobs_simple(model, imgid, tokens, tgt_vocab):\n",
    "    vecs = load_vecs([imgid])\n",
    "    encoder_final, memory_bank = encode_vecs(coco_cap.model.encoder, vecs)\n",
    "    decoder_state = model.decoder.init_decoder_state(vecs, memory_bank=memory_bank, encoder_final=encoder_final)\n",
    "\n",
    "    tgt = Variable(torch.LongTensor([tgt_vocab.stoi[tok] for tok in tokens]).unsqueeze(1).unsqueeze(1))\n",
    "    dec_out, dec_states, attn = model.decoder(tgt[:-1], memory_bank, decoder_state)\n",
    "    logits = model.generator(dec_out)\n",
    "    return F.nll_loss(logits.squeeze(1), tgt[1:].squeeze(1).squeeze(1), reduce=True, size_average=False).data[0]\n",
    "\n",
    "for_cap_0=[\n",
    "    eval_logprobs_simple(coco_cap.model, imgid, [onmt.io.BOS_WORD] + specific_captions[0].split(), tgt_field.vocab)\n",
    "    for imgid in image_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.293432235717773,\n",
       " 21.099390029907227,\n",
       " 21.52071762084961,\n",
       " 22.68265151977539,\n",
       " 20.42866325378418,\n",
       " 19.899789810180664,\n",
       " 21.961589813232422,\n",
       " 21.88710594177246,\n",
       " 17.45467185974121,\n",
       " 21.346843719482422]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_cap_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.29343  , 21.099392 , 21.520718 , 22.682652 , 20.428663 ,\n",
       "        19.899786 , 21.961592 , 21.887108 , 17.454672 , 21.346844 ],\n",
       "       [20.216093 , 21.459185 , 22.343819 , 23.25599  , 21.341995 ,\n",
       "        22.416582 , 22.364853 , 23.664122 , 17.388132 , 22.698366 ],\n",
       "       [39.53264  , 39.07738  , 36.840885 , 42.485302 , 39.422974 ,\n",
       "        38.24522  , 38.46059  , 41.825882 , 37.69344  , 39.03945  ],\n",
       "       [18.718346 , 13.342897 , 13.147852 , 13.75539  , 15.231123 ,\n",
       "        14.895325 , 16.82951  , 17.202742 , 15.470121 , 14.835234 ],\n",
       "       [15.346966 , 16.093561 , 11.594956 , 16.068754 , 13.552652 ,\n",
       "        11.984193 , 13.903265 , 14.410566 , 15.057523 , 13.432005 ],\n",
       "       [12.1750145, 13.041934 , 13.031655 , 14.190959 , 11.340078 ,\n",
       "        12.464938 , 10.303768 , 14.346598 , 13.188343 , 15.135149 ],\n",
       "       [35.806595 , 38.948387 , 37.09033  , 36.18179  , 36.957592 ,\n",
       "        35.932285 , 39.402554 , 38.134346 , 40.44112  , 37.911705 ],\n",
       "       [18.314875 , 17.092129 , 16.415756 , 16.017714 , 18.1138   ,\n",
       "        19.16465  , 18.994747 , 17.95041  , 19.27061  , 18.23053  ],\n",
       "       [30.58696  , 34.521725 , 32.751434 , 36.16851  , 29.572344 ,\n",
       "        30.790176 , 31.948643 , 31.868622 , 36.324043 , 33.81094  ],\n",
       "       [13.689573 , 11.837265 , 13.138395 , 12.988878 , 11.877425 ,\n",
       "        13.753681 , 12.883247 , 13.069333 , 14.203724 , 14.16749  ],\n",
       "       [27.554688 , 33.700684 , 27.622799 , 31.979296 , 28.47293  ,\n",
       "        24.25339  , 26.00671  , 25.573717 , 30.046585 , 29.045315 ],\n",
       "       [17.051916 , 17.935244 , 17.521046 , 18.621881 , 16.93055  ,\n",
       "        17.883041 , 17.550055 , 16.058748 , 18.720005 , 17.326118 ],\n",
       "       [51.50292  , 52.572495 , 51.914898 , 52.63201  , 51.41297  ,\n",
       "        52.26557  , 47.03139  , 49.271763 , 54.266247 , 50.955605 ],\n",
       "       [20.442703 , 20.714277 , 17.416338 , 20.721905 , 17.016356 ,\n",
       "        18.280788 , 18.245916 , 19.791233 , 20.617502 , 19.620766 ],\n",
       "       [41.282482 , 43.27955  , 43.35833  , 42.521343 , 46.150784 ,\n",
       "        44.22676  , 43.41073  , 39.02443  , 38.646015 , 41.035122 ],\n",
       "       [19.160656 , 16.024372 , 18.18378  , 16.4519   , 18.861464 ,\n",
       "        16.91659  , 19.423151 , 17.012054 , 18.282246 , 16.973587 ],\n",
       "       [24.658276 , 29.578075 , 28.508396 , 33.021366 , 26.684586 ,\n",
       "        25.540573 , 26.174694 , 27.343834 , 25.96763  , 25.522863 ],\n",
       "       [25.660383 , 30.753002 , 33.298965 , 30.875233 , 36.6471   ,\n",
       "        30.930946 , 32.903038 , 32.84001  , 24.99147  , 28.17376  ],\n",
       "       [21.918175 , 25.579655 , 24.042221 , 28.699778 , 21.90779  ,\n",
       "        21.792316 , 21.26062  , 22.200443 , 21.751287 , 21.378326 ],\n",
       "       [ 8.057874 , 10.408915 ,  9.6541395, 11.522455 , 10.840979 ,\n",
       "        12.062508 , 11.626993 ,  9.784998 , 11.002724 , 11.068507 ],\n",
       "       [ 3.377037 ,  2.3297138,  3.1559439,  2.8435092,  4.03337  ,\n",
       "         3.4973645,  3.9946084,  4.3799524,  2.401033 ,  3.05967  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_logprobs_many(model, tgt_field, imgids, texts, preproc=True, use_eos=False):\n",
    "    batch_size = len(imgids)\n",
    "    assert len(texts) == batch_size\n",
    "    \n",
    "    vecs = load_vecs(image_set)\n",
    "\n",
    "    if preproc:\n",
    "        texts = [tgt_field.preprocess(text) for text in texts]\n",
    "\n",
    "    # \"process\" handles padding and numericalization\n",
    "    tgt = tgt_field.process(texts, device=-1, train=False)\n",
    "    pad_idx = tgt_field.vocab.stoi[tgt_field.pad_token]\n",
    "    \n",
    "    if not use_eos:\n",
    "        tgt = tgt[:-1]\n",
    "    \n",
    "    encoder_final, memory_bank = encode_vecs(coco_cap.model.encoder, vecs)\n",
    "    decoder_state = model.decoder.init_decoder_state(vecs, memory_bank=memory_bank, encoder_final=encoder_final)\n",
    "    \n",
    "    # Run decoder\n",
    "    dec_out, dec_states, attn = model.decoder(\n",
    "        tgt[:-1].unsqueeze(2), # Decoder wants an extra dim for extra features.\n",
    "        memory_bank,\n",
    "        decoder_state)\n",
    "    # Run generator.\n",
    "    logits = model.generator(dec_out).contiguous()\n",
    "    seq_len, batch_size_2, num_vocab = logits.shape\n",
    "    assert batch_size == batch_size_2\n",
    "\n",
    "    # Compute nll loss, ignoring padding\n",
    "    losses = F.nll_loss(logits.view(seq_len * batch_size, num_vocab), tgt[1:].view(seq_len * batch_size), reduce=False)\n",
    "    losses = losses.view(seq_len, batch_size)\n",
    "    return losses.data.numpy().sum(axis=0)\n",
    "#     mask = tgt[1:].eq(pad_idx)\n",
    "#     losses = losses.masked_fill(mask, 0).data.sum(0)\n",
    "#     length = (~mask.data).long().sum(0)\n",
    "#     return (losses / length.float()).numpy()\n",
    "\n",
    "\n",
    "tgt_field = coco_cap.fields['tgt']\n",
    "losses_by_cap = np.array([\n",
    "    eval_logprobs_many(coco_cap.model, tgt_field, image_set, [cap] * len(image_set))\n",
    "    for cap in candidate_captions])\n",
    "losses_by_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tgt_field.preprocess(cap)) for cap in candidate_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 10)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_by_cap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgm50zUzS+j/AYIzT0VdHkGVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x162c68588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(losses_by_cap)#, xticklabels=cap_srcs_txt);\n",
    "np.argmin(losses_by_cap, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the generic caption beats *every single one* of the more specific captions for *every single image*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how about the other way around: do the specific captions match up with the image that I wrote them for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>a brown towel hanging on<br><img src=\"http://images.cocodataset.org/train2017/000000396295.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000445812.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a brown towel hangs on<br><img src=\"http://images.cocodataset.org/train2017/000000396295.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000445812.jpg\" style=\"display:inline-block;max-width: 200px\"><div>stone sink counter glass shower<br><img src=\"http://images.cocodataset.org/train2017/000000301595.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000574147.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a modern bathroom with stone<br><img src=\"http://images.cocodataset.org/train2017/000000301595.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000574147.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a shower with black and<br><img src=\"http://images.cocodataset.org/train2017/000000574147.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000259733.jpg\" style=\"display:inline-block;max-width: 200px\"><div>ledge behind open toilet seat<br><img src=\"http://images.cocodataset.org/val2017/000000102644.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000396295.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a bathroom with beige floor<br><img src=\"http://images.cocodataset.org/train2017/000000316766.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000301595.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a bathroom with cabinets under<br><img src=\"http://images.cocodataset.org/train2017/000000448613.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000362026.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a large bright white shower<br><img src=\"http://images.cocodataset.org/train2017/000000259733.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000316766.jpg\" style=\"display:inline-block;max-width: 200px\"><div>towel hanger over radiator beside<br><img src=\"http://images.cocodataset.org/train2017/000000362026.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000445812.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a dim bathroom with small<br><img src=\"http://images.cocodataset.org/train2017/000000362026.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000301595.jpg\" style=\"display:inline-block;max-width: 200px\"><div>green towel on shower door<br><img src=\"http://images.cocodataset.org/train2017/000000445812.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000396295.jpg\" style=\"display:inline-block;max-width: 200px\"><div>white towel on shower door<br><img src=\"http://images.cocodataset.org/train2017/000000480239.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000259733.jpg\" style=\"display:inline-block;max-width: 200px\"><div>a bathroom with a closed<br><img src=\"http://images.cocodataset.org/train2017/000000480239.jpg\" style=\"display:inline-block;max-width: 200px\"><img src=\"http://images.cocodataset.org/train2017/000000396295.jpg\" style=\"display:inline-block;max-width: 200px\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html=''\n",
    "for desired, actual, cap in zip([i//2 for i in range(20)], np.argmin(losses_by_cap, axis=1), candidate_captions):\n",
    "    if desired != actual:\n",
    "        html += (\n",
    "            '<div>{}<br><img src=\"{}\" style=\"display:inline-block;max-width: {}px\"><img src=\"{}\" style=\"display:inline-block;max-width: {}px\">'\n",
    "        ).format(cap, id2url[image_set[desired]], 200, id2url[image_set[actual]], 200)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, and those mismatches don't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kcarnold/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import onmt.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in images:\n",
    "    img['all_captions'] = '\\n'.join(sent['raw'].replace('\\n', ' ') for sent in img['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123287"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_images = [img for img in images if img['split'] == 'val']\n",
    "len(valid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few images have 6 sentences rather than the normal 5. We'll ignore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(96493, 6),\n",
       " (510643, 6),\n",
       " (92771, 6),\n",
       " (204311, 6),\n",
       " (567315, 6),\n",
       " (100008, 6),\n",
       " (320428, 6),\n",
       " (449312, 6),\n",
       " (459032, 6),\n",
       " (216369, 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(img['cocoid'], len(img['sentences'])) for img in valid_images if len(img['sentences']) != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25010"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_imgids = [img['cocoid'] for img in valid_images for sent in img['sentences']]\n",
    "val_caps = [sent['tokens'] for img in valid_images for sent in img['sentences']]\n",
    "len(val_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the OpenNMT models to score likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONMT models...\n",
      "coco_lm_adam_acc_46.00_ppl_16.32_e10_nooptim.pt\n",
      "Loading model parameters.\n",
      "coco_cap_adam_acc_48.73_ppl_12.56_e10_nooptim.pt\n",
      "Loading model parameters.\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "from textrec import onmt_model_2\n",
    "from textrec.paths import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.055610179901123\n",
      "1.9828158617019653\n",
      "4.326562404632568\n",
      "3.957759141921997\n",
      "2.8188583850860596\n"
     ]
    }
   ],
   "source": [
    "def eval_logprob(model, src, encoder_final, memory_bank, tgt_tokens, tgt_vocab):\n",
    "    decoder_state = coco_lm.model.decoder.init_decoder_state(src, memory_bank=memory_bank, encoder_final=encoder_final)\n",
    "    tgt_tokens = [onmt.io.BOS_WORD] + tgt_tokens + [onmt.io.EOS_WORD]\n",
    "    tgt = Variable(torch.LongTensor([tgt_vocab.stoi[tok] for tok in tgt_tokens]).unsqueeze(1).unsqueeze(1))\n",
    "    dec_out, dec_states, attn = model.decoder(tgt[:-1], memory_bank, decoder_state)\n",
    "    logits = model.generator(dec_out)\n",
    "    return F.nll_loss(logits.squeeze(1), tgt[1:].squeeze(1).squeeze(1), reduce=True).data[0]\n",
    "\n",
    "encoder_final, memory_bank = coco_lm.model.encoder(lm_src)\n",
    "for cap in val_caps[:5]:\n",
    "    print(eval_logprob(coco_lm.model, lm_src, encoder_final, memory_bank, cap, lm_tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 ms Â± 3.72 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit eval_logprob(coco_lm.model, lm_src, encoder_final, memory_bank, cap, lm_tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.05561  , 1.9828162, 4.3265624, 3.9577591, 2.8188581, 2.1887002,\n",
       "       2.9275367, 3.178268 , 3.394708 , 3.3064091, 2.8171813, 3.260916 ,\n",
       "       2.3935277, 3.4049525, 3.1746356, 1.4956913, 3.4236114, 6.401656 ,\n",
       "       2.3428607, 6.3635745, 3.0610476, 4.6592226, 4.3822927, 3.0525162,\n",
       "       3.3139853, 2.8301854, 1.9576231, 2.3530643, 1.6457199, 2.0004718,\n",
       "       1.5807973, 1.2306131], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_logprob_batched(model, src, tgt_field, encoder_final, memory_bank, batch, divide_by_length=True):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # We need to redo this each time because decoder mutates the state that's passed to it :(\n",
    "    decoder_state = coco_lm.model.decoder.init_decoder_state(src, memory_bank=memory_bank, encoder_final=encoder_final)\n",
    "    decoder_state.repeat_beam_size_times(batch_size)\n",
    "    memory_bank = memory_bank.repeat(1, batch_size, 1)\n",
    "\n",
    "    # \"process\" handles padding and numericalization\n",
    "    tgt = tgt_field.process(batch, device=-1, train=False)\n",
    "    pad_idx = tgt_field.vocab.stoi[tgt_field.pad_token]\n",
    "    \n",
    "    # Decoder wants an extra dim for extra features.\n",
    "    dec_out, dec_states, attn = model.decoder(tgt[:-1].unsqueeze(2), memory_bank, decoder_state)\n",
    "    logits = model.generator(dec_out).contiguous()\n",
    "    seq_len, batch_size_2, num_vocab = logits.shape\n",
    "    assert batch_size == batch_size_2\n",
    "    losses = F.nll_loss(logits.view(seq_len * batch_size, num_vocab), tgt[1:].view(seq_len * batch_size), reduce=False)\n",
    "    mask = tgt[1:].eq(pad_idx)\n",
    "    losses = losses.view(seq_len, batch_size).masked_fill(mask, 0).data.sum(0)\n",
    "    if divide_by_length:\n",
    "        length = (~mask.data).long().sum(0)\n",
    "        return losses / length.float()\n",
    "    else:\n",
    "        return losses\n",
    "\n",
    "encoder_final, memory_bank = coco_lm.model.encoder(lm_src)\n",
    "batch_size = 32\n",
    "unconditional_losses = eval_logprob_batched(coco_lm.model, lm_src, coco_lm.fields['tgt'], encoder_final, memory_bank, val_caps[:batch_size]).numpy()\n",
    "unconditional_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
